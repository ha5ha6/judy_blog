---
layout: single
type: posts
title:  "Soft Q-Learning"
date:   2019-12-6 17:12:25 +0900
related: true
categories: LiteratureReview
tags:
  #- Index
  - Reinforcement Learning
  - Soft-Q Learning
author:  Jiexin Wang
classes:  wide
toc: true
toc_label: "Index"
author_profile: true
---

### Background

keywords:  
- energy-based policies <- boltzmann distribution
- max-entropy policies
- amortized stein variational gradient descent

DRL:  
a promising direction for autonomous acquisition of complex behaviors  
(+) can process complex sensory input  
(+) so that can acquire elaborate behavior skills using general-purpose neural network representations

However,  
(-) most DRL methods operate on the conventional deterministic notion of optimality, where the optimal solution, at least under full observability, is always a deterministic policy  

Stochastic policies are desirable **for exploration**, usually heuristically:  
- by injecting noise  
- by initializing a stochastic policy with high entropy    

Sometime stochastic behaviors are required, the reasons are:  
- exploration in the presence of multimodel objectives
- compositionality attained via pretraining  

Other benefits:  
- robustness in the face of uncertain dynamics
- imitation learning  
- improved convergence and computational properties  
- multi-modality application  

Goal:  
must define an objective that promotes stochasticity   

A stochastic policy emerges as the optimal answer when we consider **the connection between optimal control and probabilistic inference**. [2]  

Framing **control as inference** produces policies that aim to capture   
- the single deterministic behavior that has the lowest cost
- the entire range of low-cost behaviors, explicitly maximizing the entropy of the corresponding policy  

**Instead of learning the best way to perform the task, the resulting policies try to learn all of the ways of performing the task.**

The resulting policy can serve as:    
(+) a good initialization for finetuning to a more specific behavior (learn separate running and bounding skills)  
(+) a better exploration mechanism for seeking out the best mode in a multimodel reward landscape  
(+) a more robust behavior in the face of adversarial perturbations

**Related methods** for solving such maximum entropy stochastic policy learning problems:  
- Z-learning  
- max-entropy inverse RL
- approximate inference using message passing  
- Ψ-learning
- G-learning  
- PGQ (Combining policy gradient and Q-learning)

Problems of the related methods:  
(-) operate on simple tabular representations  
(-) employ a simple parametric representation of the policy distribution auch as a conditional Gaussian  

**Therefore, the resulting distribution is limited in terms of its representational power even if the parameters of the distribution are represented by an expressive function approximator such as neural network**

The question:  
**How can we extend the framework of max-entropy policy search to arbitrary policy distributions?**  

Solution:  
- energy-based models (EBM)   

which can reveal connections between Q-learning, actor-critic and probabilistic inference  

The proposed methods:

- formulate **a stochastic policy as a EBM**, with the energy function corresponding to the "soft" Q-function obtained when optimizing the max-entropy objective

difficulty:  
(-) in high dimensional continuous spaces, sampling from the EBM policy is intractable  

- devise an approximate sampling procedure based on training **a separate sampling network**, which is optimized to produce unbiased samples from the policy EBM
- the sampling network can then be used both for updating the EBM and for action selection

in actor-critic, actor <- the sampling network  
=> **entropy regularized actor-critic can be viewed as approximate Q-learning, with the actor serving the role of an approximate sampler from an intractable posterior**

Related methods:  
- deterministic policy gradient (DPG)
- normalized advantage functions (NAF)
- policy gradient Q-learning (PGQ)

Contributions:  
- a tractable efficient algorithm for optimizing **arbitrary multimodal stochastic policies represented by energy-based models**  
- a discussion that relates this method to others in RL and probabilistic inference  

Experiment results:  
- show improved exploration performance in tasks with multi-model reward landscapes  
- a degree of compositionality in RL, by showing that stochastic energy-based policies can serve as a much better initialization for learning new skills  










### Refs

[1]Haarnoja, T., Tang, H., Abbeel, P. and Levine, S., 2017, August. Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1352-1361). JMLR. org.  

**Control and Inference**

[2]Todorov, E. General duality between optimal control and estimation. In IEEE Conf. on Decision and Control, pp. 4286–4292. IEEE, 2008.  
[3]Toussaint, M. Robot trajectory optimization using approx- imate inference. In Int. Conf. on Machine Learning, pp. 1049–1056. ACM, 2009.  

**Policy Gradient and Q-learning**  

O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. arXiv preprint arXiv:1611.01626, 2016.
