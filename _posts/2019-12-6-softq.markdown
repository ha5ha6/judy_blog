---
layout: single
type: posts
title:  "Soft Q-Learning"
date:   2019-12-6 17:12:25 +0900
related: true
categories: Literature
tags:
  #- Index
  - Reinforcement Learning
  - Soft-Q Learning
author:  Jiexin Wang
classes:  wide
toc: true
toc_label: "Index"
author_profile: true
---

### Background

keywords:  
- energy-based policies <- boltzmann distribution
- max-entropy policies
- amortized stein variational gradient descent

DRL:  
a promising direction for autonomous acquisition of complex behaviors  
(+) can process complex sensory input  
(+) so that can acquire elaborate behavior skills using general-purpose neural network representations

However,  
(-) most DRL methods operate on the conventional deterministic notion of optimality, where the optimal solution, at least under full observability, is always a deterministic policy  

Stochastic policies are desirable **for exploration**, usually heuristically:  
- by injecting noise  
- by initializing a stochastic policy with high entropy    

Sometime stochastic behaviors are required, the reasons are:  
- exploration in the presence of multimodel objectives
- compositionality attained via pretraining  

Other benefits:  
- robustness in the face of uncertain dynamics
- imitation learning  
- improved convergence and computational properties  
- multi-modality application  

Goal:  
must define an objective that promotes stochasticity   

A stochastic policy emerges as the optimal answer when we consider **the connection between optimal control and probabilistic inference**. [2]  

Framing **control as inference** produces policies that aim to capture   
- the single deterministic behavior that has the lowest cost
- the entire range of low-cost behaviors, explicitly maximizing the entropy of the corresponding policy  

**Instead of learning the best way to perform the task, the resulting policies try to learn all of the ways of performing the task.**

The resulting policy can serve as:    
(+) a good initialization for finetuning to a more specific behavior (learn separate running and bounding skills)  
(+) a better exploration mechanism for seeking out the best mode in a multimodel reward landscape  
(+) a more robust behavior in the face of adversarial perturbations

**Related methods** for solving such maximum entropy stochastic policy learning problems:  
- Z-learning  
- max-entropy inverse RL
- approximate inference using message passing  
- Œ®-learning
- G-learning  
- PGQ (Combining policy gradient and Q-learning)

Problems of the related methods:  
(-) operate on simple tabular representations  
(-) employ a simple parametric representation of the policy distribution auch as a conditional Gaussian  

**Therefore, the resulting distribution is limited in terms of its representational power even if the parameters of the distribution are represented by an expressive function approximator such as neural network**

The question:  
**How can we extend the framework of max-entropy policy search to arbitrary policy distributions?**  

Solution:  
- energy-based models (EBM)   

which can reveal connections between Q-learning, actor-critic and probabilistic inference  

The proposed methods:

- formulate **a stochastic policy as a EBM**, with the energy function corresponding to the "soft" Q-function obtained when optimizing the max-entropy objective

difficulty:  
(-) in high dimensional continuous spaces, sampling from the EBM policy is intractable  

- devise an approximate sampling procedure based on training **a separate sampling network**, which is optimized to produce unbiased samples from the policy EBM
- the sampling network can then be used both for updating the EBM and for action selection

in actor-critic, actor <- the sampling network  
=> **entropy regularized actor-critic can be viewed as approximate Q-learning, with the actor serving the role of an approximate sampler from an intractable posterior**

Related methods:  
- deterministic policy gradient (DPG)
- normalized advantage functions (NAF)
- policy gradient Q-learning (PGQ)

Contributions:  
- a tractable efficient algorithm for optimizing **arbitrary multimodal stochastic policies represented by energy-based models**  
- a discussion that relates this method to others in RL and probabilistic inference  

Experiment results:  
- show improved exploration performance in tasks with multi-model reward landscapes  
- a degree of compositionality in RL, by showing that stochastic energy-based policies can serve as a much better initialization for learning new skills  

### Context

**MaxEnt RL problem formulation**

![](/assets/images/maxentrl.png){:width="80%"}

Note that, the MaxEnt objective differs qualitatively from  
- Boltzmann Exploration  
- PGQ  

BE and PGQ greedily maximize entropy at the current time step, but do not explicitly optimize for policies that aim to reach states where they will have high entropy in their future  
While the MaxEnt objective can maximize the entropy of the entire traj distribution for the policy pi  

**Policy Representation**  

prior work:  
- discrete multinomial distributions  
- Gaussian distributions  

However, **engergy-based model** is more general class of distribution that can represent complex, multimodal behaviors  

![](/assets/images/softbellman.png){:width="70%"}

### Soft Q-Learning

**Soft Q-Iteration**  

![](/assets/images/softqiter.jpg){:width="80%"}

**Practical Problems**  

- the soft Bellman backup cannot be performed exactly in continuous or large state and action spaces  
- sampling from the energy-based model is intractable in general  

**Soft Q-Learning**

Normally, since the soft Bellman backup is a contraction, the optimal value function is the fixed point of the Bellman backup  
We can find Q* by minimizing the soft Bellman error \|ùíØQ-Q\|, where ùíØ is the soft bellman operator  

However,
This procedure is intractable due to the integral in V_soft(st)  

Solution:  
Turn this procedure into a **stochastic optimization**, which leads to a SGD update  

![](/assets/images/softqstoupdate.jpg){:width="80%"}

This SGD can be updated using sampled states and actions, the sampling distribution **qs and qa** can be arbitrary    
We can use real samples from rollouts of current policy œÄ(at\|st) ‚àù exp‚Å°[1/Œ±(Q_soft^Œ∏(st,at))]  

For **qa'**, can be uniform, or can be the current policy which produces an unbiased estimate of the soft value as can be confirmed by substitution  

(-) uniform scale poorly to high dimensions  

Problem:  
In continuous spaces, we still need a tractable way to sample from the policy œÄ(at\|st) ‚àù exp‚Å°[1/Œ±(Q_soft^Œ∏(st,at))], both to take on-policy actions and to generate action samples for estimating the soft value function  
Since the form of the policy is so general, sampling from it is intractable  

Solution:  
will need to use an approximate sampling procedure  

### Approximate Sampling  

How to sample from the soft Q-function?

Two categories of sampling from **energy-based distribution**:  
- Markov chain Monte Carlo (MCMC) [Sallans & Hinton, 2004]  
- learn a strochastic sampling network trained to output approximate samples from the target distribution  [Zhao et al, 2016][Kim & Bengio, 2016]






### Refs

[1]Haarnoja, T., Tang, H., Abbeel, P. and Levine, S., 2017, August. Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1352-1361). JMLR. org.  

**Control and Inference**

[2]Todorov, E. General duality between optimal control and estimation. In IEEE Conf. on Decision and Control, pp. 4286‚Äì4292. IEEE, 2008.  
[3]Toussaint, M. Robot trajectory optimization using approx- imate inference. In Int. Conf. on Machine Learning, pp. 1049‚Äì1056. ACM, 2009.  

**Boltzmann Exploration**  

[4]Sallans, B. and Hinton, G. E. Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5(Aug):1063‚Äì1088, 2004.

**Policy Gradient and Q-learning**  

[5]O‚ÄôDonoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. arXiv preprint arXiv:1611.01626, 2016.

**Soft V and Q**  

[6]Ziebart, B. D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, 2010.  
[7]Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conf. on Uncertainty in Artificial Intelligence, 2016.
