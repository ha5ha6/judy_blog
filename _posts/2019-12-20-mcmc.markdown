---
layout: single
type: posts
title:  "Markov Chain Monte Carlo"
date:   2019-12-20 16:28:25 +0900
related: true
categories: ML-Basic
tags:
  #- Index
  - Machine Learning
author:  Jiexin Wang
classes:  wide
toc: true
toc_label: "Index"
author_profile: true
---

### Background

Markov Chain Monte Carlo refers to a class of methods for **sampling from a probability distribution in order to construct the most likely distribution**.  
For example, we cannot directly calculate the logistic distribution, so instead we generate thousands of values — called samples — for the parameters of the function to create an approximation of the distribution.  
The more samples generated, the closer and closer the approximation gets to the actual true distribution.   

**Monte Carlo**  
Monte Carlo refers to a general technique of **using repeated random samples to obtain a numerical answer**.  
It can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response.  
By choosing random values, we can explore a large portion of the parameter space, the range of possible values for the variables.  

**Markov Chain**  
A Markov Chain is **a process where the next state depends only on the current state**.  

(+) it's memoryless because only the current state matters and not how it arrived in that state.  

**Markov Chain Monte Carlo**  
MCMC is a method that **repeatedly draws random values for the parameters of a distribution based on the current values. Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters**.  
MCMC can be considered as a random walk that gradually converges to the true distribution.

### Example Problem

Given 100 data points, estimate the posterior of the mean.  
(The date is generated by Gaussian distribution mean=0, variance=1)   

```python
import numpy as np
import matplotlib.pyplot as plt

data=np.random.randn(100)
plt.hist(data)
```

![](https://ha5ha6.github.io/judy_blog/assets/images/randn100.png){:width="50%"}

Steps:  
1. assume data is normal distributed: the likelihood of the model is Gaussian with parameter μ,σ - mean and standard deviation, assume σ=1 is known for simplicity    
2. choose a prior, assume a Gaussian as prior for simplicity   

Model:

      μ ~ Gaussian (0,1)       <-   prior
    x|μ ~ Gaussian (x|μ,σ=1)   <-   likelihood


**Analytical Solution**  
Can compute the posterior analytically, because for a Gaussian likelihood with known standard deviation, the Gaussian prior for μ is conjugate  

=> the posterior for μ is Gaussian as well  

**Conjugate**  
posterior and prior will be the same distribution  

![](https://ha5ha6.github.io/judy_blog/assets/images/normalnor.jpg){:width="80%"}


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def gaussian(x,mu,sigma):
    pdf=(1./(sigma*np.sqrt(2*np.pi))*np.exp(-(x-mu)**2/(2*sigma**2)))
    return pdf

def posterior_analytical(data,mu_0,sig_0): #mu_0,sig_0 prior parameters
    n=len(data)
    sigma_like=1.
    mu_post=(mu_0/sig_0**2+data.sum()/sig_like**2)/(1./sig_0**2+n/sigma_like**2)
    sigma_post=(1./sig_0**2+n/sig_like**2)**-1
    return mu_post,sigma_post

data=np.random.randn(100)
x=np.linspace(-1,1,500)

mu_post,sig_post=posterior_analytical(data,0.,1.) #mu_0,sig_0
pdf1=gaussian(x,mu_post,np.sqrt(sig_post)) #by hand
pdf2=norm(mu_post,np.sqrt(sig_post)).pdf(x) #by library, same way to get pdf

plt.plot(x,pdf1,color='r',linewidth=2)
plt.plot(x,pdf2,'g',linewidth=2)
```

![](https://ha5ha6.github.io/judy_blog/assets/images/randn100_post.png){:width="50%"}



### References

[Markov Chain Monte Carlo in Python by Will Koehrsen](https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98)  
[Markov Chain](https://brilliant.org/wiki/markov-chains/)  
[Monte Carlo theory, methods and examples](http://statweb.stanford.edu/~owen/mc/)  
[Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)  
