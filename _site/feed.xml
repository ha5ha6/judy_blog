<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-03-04T11:41:11+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiexin Wang</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Jiexin Wang</name></author><entry><title type="html">Variational Inference</title><link href="http://localhost:4000/ml-basic/2020/03/03/variationalinference.html" rel="alternate" type="text/html" title="Variational Inference" /><published>2020-03-03T22:28:25+09:00</published><updated>2020-03-03T22:28:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/03/03/variationalinference</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/03/03/variationalinference.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;https://zhiyzuo.github.io/VI/#a-motivating-example&lt;/p&gt;

&lt;p&gt;https://www.slideshare.net/PeadarCoyle/variational-inference-in-python&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><summary type="html">Definition</summary></entry><entry><title type="html">Hidden Markov Model</title><link href="http://localhost:4000/ml-basic/2020/03/03/hmm.html" rel="alternate" type="text/html" title="Hidden Markov Model" /><published>2020-03-03T22:28:25+09:00</published><updated>2020-03-03T22:28:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/03/03/hmm</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/03/03/hmm.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9&lt;/p&gt;

&lt;p&gt;https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><summary type="html">Definition</summary></entry><entry><title type="html">Entropy &amp;amp; KL Divergence</title><link href="http://localhost:4000/ml-basic/2020/03/02/kl.html" rel="alternate" type="text/html" title="Entropy &amp; KL Divergence" /><published>2020-03-02T18:25:25+09:00</published><updated>2020-03-02T18:25:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/03/02/kl</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/03/02/kl.html">&lt;h3 id=&quot;entropy&quot;&gt;Entropy&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt; gives us a way to quantify the information content of a given probability distribution&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         n
H(X) = - Σ p(x_i) log p(x_i)
        i=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Suppose we have a simple probability distribution over the likelihood of a coin flip resulting in heads or tails [p,1-p]&lt;/p&gt;

&lt;p&gt;Plugging [p,1-p] into H(X), we have&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(X) = - [plogp+(1-p)log(1-p)]

p=0.5  H(x)=0.69   entropy peak high   more info
p=0.9  H(x)=0.32
p-&amp;gt;1   H(x)-&amp;gt;0     entropy low         no info    - events that always occur contain no info
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Return to definition of H(X):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         n                     n              1
H(X) = - Σ p(x_i) log p(x_i) = Σ p(x_i) log -----
        i=1                   i=1           p(x_i)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cross-entropy&quot;&gt;Cross Entropy&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Cross Entropy&lt;/strong&gt; is used to quantify the information content of one distribution p relative to another q&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                                             1
H(p,q) = - Σ p(x_i) log q(x_i) = Σ p(x) log ----
          x_i                    x          q(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;kl-divergence&quot;&gt;KL Divergence&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;KL divergence&lt;/strong&gt; (aka &lt;strong&gt;relative entropy&lt;/strong&gt;) is a distance metric that quantifies the difference between two probability distributions&lt;/p&gt;

&lt;p&gt;Useful in measuring loss in machine learning, related to &lt;strong&gt;cross-entropy&lt;/strong&gt;&lt;br /&gt;
Useful in dealing with &lt;strong&gt;a complex distribution scenario&lt;/strong&gt;:&lt;br /&gt;
Rather than working with the distribution directly, we can use another distribution with well known properties (i.e. Gaussian) that does a decent job of describing the data&lt;/p&gt;

&lt;p&gt;Simply put, we can use the KL divergence to tell whether a poisson distribution or a normal distribution is a better one at approximating the data&lt;/p&gt;

&lt;p&gt;KL divergence is also a key component of &lt;strong&gt;Gaussian Mixture Models&lt;/strong&gt; and &lt;strong&gt;t-SNE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall entropy and cross entropy:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(p,q) - the amount of information needed to encode p and q
H(p) - the amount of information necessary to encode p
KL(p||q) - the amount of info needed to encode p with q minus the amount of info to encode p

                                                                       p(x)
KL(p||q) = H(p,q) - H(p) = - Σ p(x)logq(x) + Σ p(x)logp(x) = Σ p(x)log ----
                             x               x               x         q(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For distributions p and q of a &lt;strong&gt;discrete random variable&lt;/strong&gt;,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                       p(x)
 KL(p||q) = Σ p(x) log ---- dx
                       q(x)                   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For distributions p and q of a &lt;strong&gt;continuous random variable&lt;/strong&gt;,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                      p(x)
KL(p||q) = ∫ p(x) log ---- dx
                      q(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;KL divergence is not symmetrical: KL(p||q)!=KL(q||p)&lt;/li&gt;
  &lt;li&gt;the lower the KL divergence, the closer the two distributions are to one another&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python-examples&quot;&gt;python Examples&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;For discrete cases&lt;/strong&gt;, generate red ball 40%, and blue ball 60%&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_balls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#generate uni random in [0.0, 1.0)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;percentage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#fix the output&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_balls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_balls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt1000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_balls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;585&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;415&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p1000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.585&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.415&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#handcraft&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kl10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.020410997260127586&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kl100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001860706678335388&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kl1000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0004668811751176276&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#scipy lib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.020410997260127586&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001860706678335388&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0004668811751176276&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;For continuous cases&lt;/strong&gt;, taking 2 Gaussian &lt;br /&gt;
p=N(0,2) and q=N(2,2) as example, their KL(p||q)=500;&lt;br /&gt;
p=N(0,2) and q=N(5,4) as example, their KL(p||q)=1099:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#sns.set()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#to avoid log 0, cuz log 0=-inf&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#np.where means if p!=0, run p*np.log(p/q), else 0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(p||q)=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%1.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p with m=0,std=2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'q with m=2,std=2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'kl_norm2.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(p||q)=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%1.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p with m=0,std=2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'q with m=5,std=4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'kl_norm2_.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_norm2_0.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt; &lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_norm2_1.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of KLs of covering two-modal Gaussian and one-model Gaussian&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Figure 1 with q1=N(0,4) that covers both two modal has the lowest value of KL(p||q1)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_2modal_0.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt; &lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_2modal_1.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_2modal_2.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt; &lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_2modal_3.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;minimizing-kl-divergence&quot;&gt;Minimizing KL Divergence&lt;/h3&gt;

&lt;p&gt;Since &lt;strong&gt;the lower the KL divergence, the closer the two distributions are to one another&lt;/strong&gt;, we can estimate the Gaussian parameters for example of one distribution by minimizing its KL divergence w.r.t another&lt;/p&gt;

&lt;p&gt;Using &lt;strong&gt;Gradient Descent&lt;/strong&gt;&lt;br /&gt;
Create p=N(0,2), q=N(mu,sig) (mu,sig with random parameters)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#(20000,) reshape to (1,20000)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#tensorflow memory relocate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;traj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sig2s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;traj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sig2s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig2s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'q approach to p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'final KL(p||q)=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%1.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;traj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_pdf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'kl_gd.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;traj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'episodes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(p||q) values'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'kl_klvalue.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_gd.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt; &lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/kl_klvalue.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extensions&quot;&gt;Extensions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;MaxEnt RL&lt;/li&gt;
  &lt;li&gt;Variational inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810&quot;&gt;KL Divergence Python Example&lt;/a&gt;  &lt;br /&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;wiki&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.pedro.ai/blog/2017/02/22/entropy-and-kl-divergence/&quot;&gt;Entropy and KL&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><summary type="html">Entropy</summary></entry><entry><title type="html">Maximum Entropy RL</title><link href="http://localhost:4000/rl-basic/2020/03/02/maxentrl.html" rel="alternate" type="text/html" title="Maximum Entropy RL" /><published>2020-03-02T14:45:25+09:00</published><updated>2020-03-02T14:45:25+09:00</updated><id>http://localhost:4000/rl-basic/2020/03/02/maxentrl</id><content type="html" xml:base="http://localhost:4000/rl-basic/2020/03/02/maxentrl.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;Randomness is for exploration&lt;/p&gt;

&lt;p&gt;a ~ π(a|s) - policy is defined by a probability distribution, in many RL algorithms&lt;/p&gt;

&lt;p&gt;For discrete actions, picking one of many possible actions, a categoritcal distribution is used (left fig)  &lt;br /&gt;
For continuous actions, a Gaussian with a mean and a std may be used (right fig)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/entropy.png&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With these kinds of policies, the randomness of the actions an agent takes can be quantified by the &lt;strong&gt;entropy&lt;/strong&gt; of that probability distribution&lt;/p&gt;

&lt;p&gt;The greater the entropy, the more random the actions an agent takes&lt;br /&gt;
High entropy - more disorer (blue lines in the fig), Low entropy (orange lines in the fig)&lt;/p&gt;

&lt;p&gt;Entropy of a discrete probability distribution p:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(X) = E [I(X)] = - Σ p(x)logp(x)
        X          x∈X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;RL process will naturally lead to the entropy of the action selection policy decreasing&lt;/strong&gt; (from blue lines to oranges lines), i.e. annealing of a Boltzmann softmax policy&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encouraging Entropy&lt;/strong&gt;&lt;br /&gt;
It is also typical to add “entropy bonus” to the loss function to encourage the agent to take actions more unpredictably (with higher randomness)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Entropy Bonus&lt;/strong&gt;:&lt;br /&gt;
(+) avoid an agent quickly converges to a local, but not necessarily globally optimal policy&lt;/p&gt;

&lt;p&gt;Similar as optimizing for the long-term sum of future rewards, we can also optimize for the long-term sum of entropy:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It is optimal for an agent to learn not only to get as many future rewards as possible, but also to put itself in positions where its future entropy will be the largest!!&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                ∞
π* = argmax E  [Σ γ^t (r+αH^π)]
        π    π t=0     |t |t  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;High entropy also means adaptive to env changes: keeping pre-existing ways or trying new ways&lt;br /&gt;
&lt;strong&gt;The key is to plan not only for a good outcome, but the ability to change when the world does!!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High entropy policy&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(π(.|s)) = - Σ π(a|s)log π(a|s) = E [ -log π(a|s)]
              a                   a~π(.|s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(+) high entropy policy means higher disorder in policy&lt;br /&gt;
(+) try new risky behaviors &amp;lt;=&amp;gt; potentially explore unexplored regions&lt;/p&gt;

&lt;p&gt;Standard MDP:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;max E [Σ γ^t r(st,at)]
 π   π t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;MaxEnt MDP:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;max E {Σ γ^t [r(st,at)+H(π(.|st))]}
 π   π t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Theorem 1:&lt;br /&gt;
Soft Q-function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                             ∞
Q^π (st,at) ≜ r(st,at) + E   Σ [γ^l (r(s  , a) + H(π(.|s))]
soft                      π l=1        |t+l |t+l       |t+l
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Soft V-function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;V^π (st) ≜ log ∫ exp(Q^π (st,a)) da
soft           A     soft
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optimal value functions:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q* (st,at) ≜ max Q^π (st,at)
soft          π  soft

V* (st) ≜ log ∫ exp(Q* (st,a)) da
soft                soft
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;soft-update-derivation&quot;&gt;Soft Update Derivation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/softq_de1.jpeg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/softq_de2.jpeg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/softq_de3.jpeg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/softq_de4.jpeg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@awjuliani/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d&quot;&gt;Maximum Entropy Policies in Reinforcement Learning &amp;amp; Everyday Life&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/&quot;&gt;Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Reinforcement Learning" /><summary type="html">Definition</summary></entry><entry><title type="html">Importance Sampling</title><link href="http://localhost:4000/ml-basic/2020/02/21/importancesampling.html" rel="alternate" type="text/html" title="Importance Sampling" /><published>2020-02-21T17:18:25+09:00</published><updated>2020-02-21T17:18:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/02/21/importancesampling</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/02/21/importancesampling.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/importance-sampling-introduction-e76b2c32e744&quot;&gt;Importance Sampling Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Later check!
&lt;a href=&quot;http://people.duke.edu/~ccc14/sta-663-2016/16A_MCMC.html&quot;&gt;Metropolis and Gibbs Sampling&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><summary type="html">Definition</summary></entry><entry><title type="html">Machine Learning Index</title><link href="http://localhost:4000/ml-basic/2020/02/19/mlbasic.html" rel="alternate" type="text/html" title="Machine Learning Index" /><published>2020-02-19T17:29:25+09:00</published><updated>2020-02-19T17:29:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/02/19/mlbasic</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/02/19/mlbasic.html">&lt;h3 id=&quot;basic&quot;&gt;Basic&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/03/02/kl.html&quot;&gt;&lt;strong&gt;KL Divergence&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;sampling&quot;&gt;Sampling&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/21/importancesampling.html&quot;&gt;&lt;strong&gt;Importance Sampling&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2019/12/20/mcmc.html&quot;&gt;&lt;strong&gt;MCMC&lt;/strong&gt;&lt;/a&gt; includes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2019/12/20/mcmc.html#metropolis-hastings&quot;&gt;Metropolis-Hastings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/18/gibbssampling.html&quot;&gt;Gibbs Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2019/12/20/mcmc.html&quot;&gt;&lt;strong&gt;MCMC&lt;/strong&gt;&lt;/a&gt; - a method that repeatedly draws random values for the parameters of a distribution based on the current values. Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MCMC&lt;/strong&gt; can be considered as a random walk that gradually converges to the true distribution&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2019/12/20/mcmc.html#metropolis-hastings&quot;&gt;&lt;strong&gt;Metropolis–Hastings&lt;/strong&gt;&lt;/a&gt; is a MCMC method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/18/gibbssampling.html&quot;&gt;&lt;strong&gt;Gibbs Sampling&lt;/strong&gt;&lt;/a&gt; is a MCMC method for obtaining a sequence of observations which are approximately from a specified multivariate probability distribution, when direct sampling is difficult&lt;/p&gt;

&lt;h3 id=&quot;stochastic-optimization&quot;&gt;Stochastic Optimization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2019/12/18/stochasticoptimization.html&quot;&gt;&lt;strong&gt;Stochastic optimization&lt;/strong&gt;&lt;/a&gt; (SO) methods are optimization methods for minimizing or maximizing an objective function when randomness is present&lt;/p&gt;

&lt;p&gt;SO Includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stochastic Gradient Descent&lt;/li&gt;
  &lt;li&gt;Mini-Batch Stochastic Gradient Descent&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;energy-based-model&quot;&gt;Energy-based Model&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/05/energybasedmodel.html&quot;&gt;&lt;strong&gt;Energy-based Model&lt;/strong&gt;&lt;/a&gt; includes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/10/RBM.html&quot;&gt;Restricted Boltzmann Machines (RBM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/05/energybasedmodel.html&quot;&gt;&lt;strong&gt;Energy-based Model&lt;/strong&gt;&lt;/a&gt; (EBM) captures dependencies by associating a scalar energy (a measure of compatibility) to each configuration of the variables&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ha5ha6.github.io/judy_blog/ml-basic/2020/02/10/RBM.html&quot;&gt;&lt;strong&gt;Restricted Boltzmann Machine&lt;/strong&gt;&lt;/a&gt; is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RBMs&lt;/strong&gt; are a variant of &lt;strong&gt;Boltzmann machines&lt;/strong&gt;, with the restriction that their neurons must form a bipartite graph: a pair of nodes from each of the two groups of units (commonly referred to as the “visible” and “hidden” units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RBM&lt;/strong&gt; related:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the gradient-based &lt;strong&gt;Contrastive Divergence algorithm&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Deep Belief Networks (stacking RBMs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;unsupervised&quot;&gt;Unsupervised&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RBMs&lt;/li&gt;
  &lt;li&gt;Autoencoder&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><category term="Index" /><summary type="html">Basic</summary></entry><entry><title type="html">Gibbs Sampling</title><link href="http://localhost:4000/ml-basic/2020/02/18/gibbssampling.html" rel="alternate" type="text/html" title="Gibbs Sampling" /><published>2020-02-18T17:52:25+09:00</published><updated>2020-02-18T17:52:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/02/18/gibbssampling</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/02/18/gibbssampling.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;In statistics, &lt;strong&gt;Gibbs sampling&lt;/strong&gt; or a &lt;strong&gt;Gibbs sampler&lt;/strong&gt; is a &lt;strong&gt;Markov chain Monte Carlo (MCMC)&lt;/strong&gt; algorithm for obtaining a sequence of observations which are approximately from a specified multivariate probability distribution, when direct sampling is difficult.&lt;/p&gt;

&lt;p&gt;This sequence can be used&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;to approximate the joint distribution (i.e. to generate a histogram of the distribution)&lt;/li&gt;
  &lt;li&gt;to approximate the marginal distribution of one of the variables, or some subset of the variables (i.e. the unknown parameters or latent variables)&lt;/li&gt;
  &lt;li&gt;to compute an integral (i.e. the expected value of one of the variables)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.&lt;/p&gt;

&lt;p&gt;Gibbs Sampling is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;commonly used as a means of statistical inference, especially &lt;strong&gt;Bayesian inference&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;a randomized algorithm&lt;/strong&gt; (i.e. an algorithm that makes use of random numbers)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;an alternative to deterministic algorithms&lt;/strong&gt; for statistical inference (i.e. EM algorithm)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;line-fitting&quot;&gt;Line Fitting&lt;/h2&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Given: data {x_i}=X, {y_i}=Y and uncertainties {σ_i} (underlying model y=ax+b+σ) &lt;br /&gt;
Goal: find parameters a,b using Gibbs sampling&lt;/p&gt;

&lt;p&gt;Define &lt;strong&gt;the posterior&lt;/strong&gt; distribution of our model parameters a,b:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;            p(Y|a,b)p(a,b)
p(a,b|Y) = ----------------
                 p(Y)

p(Y) can be dropped since it's fixed

p(a,b|Y) ∝ p(Y|a,b)p(a,b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The uncertainties are Gaussian so &lt;strong&gt;the likelihood p(Y|a,b)&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                  1              (y_i-ax_i-b)^2
p(Y|a,b) = ∏ ------------- exp(- ---------------)
           i sqrt(2πσ_i^2)           2σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assume &lt;strong&gt;a prior&lt;/strong&gt; for a,b:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(a,b) ∝ 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now &lt;strong&gt;the posterior&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                    (y_i-ax_i-b)^2
p(a,b|Y) ∝ exp(- Σ ---------------)
                       2σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Gibbs Sampling relies on sampling from the conditional distribution for each parameter in turn&lt;/strong&gt;, like:&lt;br /&gt;
p(a|b,Y), p(b|a,Y)&lt;/p&gt;

&lt;p&gt;(+) can be useful when it is very difficult or impossible to sample from the joint distribution of all model parameters&lt;br /&gt;
(+) is only useful if the conditional distribution in a form which is easy to directly sample from without rejection, i.e. Gaussian&lt;/p&gt;

&lt;p&gt;The conditional distribution by treating all other model parameters as fixed:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                   y_i^2+a^2x_i^2+b^2-2ax_iy_i-2y_ib+2abx_i             a^2x_i^2-2ax_iy_i+2abx_i
p(a|b,Y) ∝ exp(- Σ -----------------------------------------) ∝ exp(- Σ ------------------------)
                 i                   2σ_i^2                           i         2σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: only take a, and omit other parameters like b and other variables like y_i,x_i&lt;/p&gt;

&lt;p&gt;Turn the proportional result of p(a|b,Y) into Gaussian form:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                        x_i^2
p(a|b,Y) ∝ exp(- 1/2 Σ (-----) (a-μ_a)^2)
                     i  σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a Gaussian with mean and std:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         x_iy_i          x_i
      Σ (------) - b Σ (------)
      i   σ_i^2      i   σ_i^2
μ_a = --------------------------
               x_i^2
            Σ (------)
            i  σ_i^2

          x_i^2
σ_a = [Σ (------)]^(-1/2)
       i  σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similarly, p(b|a,Y)’s mean and std:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;           y_i           x_i
      Σ (------) - a Σ (------)
      i   σ_i^2      i   σ_i^2
μ_b = --------------------------
                   1
              Σ (------)
              i  σ_i^2

            1
σ_b = [Σ (------)]^(-1/2)
       i  σ_i^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#1~10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# sigma, known&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;black&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errorbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'generated data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gibbs_data.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#theta=[a,b]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu_a_no&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu_a_de&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;mu_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu_a_no&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu_a_de&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sig_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_a_de&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sig_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_a&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;smaple_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu_b_no&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu_b_de&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;mu_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu_b_no&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu_b_de&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sig_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_b_de&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sig_b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu_b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#start sampling  &lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#generate [0,1] [1,0] in random order&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#randomized the order to increase efficiency&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'samples of a,b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'true a,b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gray&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gibbs_sample.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'samples of a dist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'real a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gibbs_a.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'samples of b dist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'real b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gibbs_b.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#posterior quantiles from 16%~84%&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a_samples_burn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b_samples_burn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y_post&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_samples_burn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_samples_burn&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_quantiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;84&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_quantiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;navajowhite&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;black&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errorbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_errs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gibbs_post.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/gibbs_data.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/gibbs_sample.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/gibbs_a.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/gibbs_b.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1-std posterior:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/gibbs_post.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gibbs_sampling&quot;&gt;wiki&lt;/a&gt;
&lt;a href=&quot;https://accarnall.github.io/gibbs_sampling_straight_line/&quot;&gt;Fitting a straight line to data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Later check!
&lt;a href=&quot;http://people.duke.edu/~ccc14/sta-663-2016/16A_MCMC.html&quot;&gt;Metropolis and Gibbs Sampling&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Machine Learning" /><category term="sampling" /><category term="MCMC" /><summary type="html">Definition</summary></entry><entry><title type="html">Restricted Boltzmann Machine</title><link href="http://localhost:4000/ml-basic/2020/02/10/RBM.html" rel="alternate" type="text/html" title="Restricted Boltzmann Machine" /><published>2020-02-10T15:13:25+09:00</published><updated>2020-02-10T15:13:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/02/10/RBM</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/02/10/RBM.html">&lt;h3 id=&quot;energy-based-model&quot;&gt;Energy-based Model&lt;/h3&gt;

&lt;p&gt;EBM defines a probability distribution through an Energy function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        exp(-E(x))      exp(-E(x))
p(x) = -----------  =  -----------
            Z          Σ exp(-E(x))
                       x

Z = Σ exp(-E(x))
    x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;An EBM can be learned by performing SGD on the empirical negative log-likelihood of the training data&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L(θ,D) = 1/N Σ    log p(x_i)   &amp;lt;- log-likelihood
             x_i∈D

l(θ,D) = - L(θ,D)   &amp;lt;- loss function  

                        ∂log p(x_i)
stochastic gradient = - ------------
                             ∂θ

θ - model parameters  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In many cases, we do not observe the example x fully, or we want to introduce some non-observed variables to increase the expressive power of the model  &lt;br /&gt;
=&amp;gt; We consider &lt;strong&gt;an observed part x&lt;/strong&gt; and &lt;strong&gt;a hidden part h&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                    exp(-E(x,h))
p(x) = Σ p(x,h) = Σ ------------
       h          h       Z
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We define &lt;strong&gt;Free Energy&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;F(x) = - log Σ exp(-E(x,h))
             h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        exp(-F(x))     exp(-F(x))
p(x) = ------------ = ------------
            Z         Σ exp(-F(x))
                      x

the negative       ∂log p(x)    ∂F(x)            ∂F(x~)
log-likelihood = - ---------- = ----- - Σ  p(x~) ------
gradient               ∂θ         ∂θ    x~         ∂θ

                 ∂F(x)
positive phase = -----
                  ∂θ
                            ∂F(x~)
negative phase = - Σ  p(x~) ------
                   x~         ∂θ
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The terms ‘positive’ and ‘negative’ reflect their effect on the probability density defined by the model&lt;/p&gt;

&lt;p&gt;Positive phase increases the probability of training data (by reducing the corresponding free energy)&lt;br /&gt;
Negative phase decreases the probability of samples generated by the model&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;the negative       ∂log p(x)    ∂F(x)               ∂F(x~)
log-likelihood = - ---------- ≈ ----- - 1/|N| Σ     ------
gradient               ∂θ         ∂θ          x~∈N    ∂θ

N - negative particles  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where we could ideally like element x~ from N to be sampled according to p (i.e. Monte-Carlo)&lt;/p&gt;

&lt;p&gt;The point is &lt;strong&gt;how to extract these negative particles N&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The statistical literature abounds with sampling methods, i.e. MCMC + RBM&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;boltzmann-machines&quot;&gt;Boltzmann Machines&lt;/h3&gt;

&lt;p&gt;Boltzmann distribution aka Gibbs Distribution is from Thermodynamics, that’s why it’s called Energy-based Models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Boltzmann Machines&lt;/strong&gt; are a particular form of &lt;strong&gt;log-linear Markov Random Field (MRF)&lt;/strong&gt;, i.e. for which the energy function is linear in its free parameters.&lt;/p&gt;

&lt;p&gt;To make them powerful enough to represent complicated distributions, we consider that some of the variables are never observed -&amp;gt; &lt;strong&gt;hidden variables (aka hidden units)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;By having more hidden variables, we can increase the modeling capacity of the Boltzmann Machine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Boltzmann Machines&lt;/strong&gt; are models with only hidden and visible nodes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;non-deterministic (stochastic)&lt;/li&gt;
  &lt;li&gt;deep generative models&lt;/li&gt;
  &lt;li&gt;unsupervised deep learning (only use inputs X, no labeling for the data)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;No output nodes!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the nodes are connected to all other nodes irrespective of whether they are input or hidden nodes.&lt;br /&gt;
This allows them to share information among themselves and self-generate subsequent data.&lt;/p&gt;

&lt;h3 id=&quot;restricted-boltzmann-machines&quot;&gt;Restricted Boltzmann Machines&lt;/h3&gt;

&lt;p&gt;RBM restricts Boltzmann Machines to those without visible-visible and hidden-hidden connections&lt;/p&gt;

&lt;p&gt;Two-layered NN with generative capabilities, which are connected by a fully bipartite graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;visible layer (only connected to hidden layer)&lt;/li&gt;
  &lt;li&gt;hidden layer (only connected to visible layer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bias units, the stochastic part, which is the different part from autoencoder&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;visible bias - reconstruct the input during a backward pass&lt;/li&gt;
  &lt;li&gt;hidden bias - produce the activation on the forward pass&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(+) a symmetric bipartite graph&lt;br /&gt;
(+) multiple RBMs can be stacked and fine-tuned through GD and BP &lt;br /&gt;
(+) can learn a probability distribution&lt;br /&gt;
(-) being replaced by GAN or VA&lt;/p&gt;

&lt;p&gt;RBM:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         o
   o  /  o
v  o --- o  h
   o  \  o
      W  o

   +     +
   a_i   b_j

v - visible layer (binary units)
h - hidden layer (binary units)
a - bias for v
b - bias for h
W - weights  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Energy function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;E(v,h) = - h'Wv - a'v - b'h
       = - Σ Σ W * h * v - Σ a * v - Σ b * h
           j i |ji |j  |i  i |i  |i  j |j  |j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Free Energy:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;F(v) = - Σ log Σ  exp(h_j(b_j+Wv))) - a'v
         j     h_j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Property of RBM:&lt;br /&gt;
visible and hidden units are conditionally independent given one-another&lt;/p&gt;

&lt;p&gt;so we can write:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(h|v) = ∏ p(h_j|v)
         j

p(v|h) = ∏ p(v_i|h)
         i
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Distribution:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(v,h) = exp ( -E(v,h) ) / Z

Z - partition function (intractable)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;High-energy associated to low probability!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RBMs with binary units&lt;/strong&gt;&lt;br /&gt;
Binary units: v_i and h_j ∈ {0,1}&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(h_j=1|v) = sig (b_j+Wv)
p(v_i=1|h) = sig (a_i+hW')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The free energy can be:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;F(v) = - Σ log (1+exp(b_j+Wv)) - a'v
         j
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then the gradients are:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ∂log p(v)
- ---------- = E_v [p(h_j|v) * v_i] - v_ij * sig (Wv + b_j)
    ∂W_ij

  ∂log p(v)
- ---------- = E_v [p(h_j|v)] - sig (Wv)
    ∂a_i

  ∂log p(v)
- ---------- = E_v [p(v_i|h)] - v_ij
    ∂b_j

            exp(x)
sig(x) = ------------
          1 + exp(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Sampling&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Samples of p(x) can be obtained by running &lt;strong&gt;a Markov chain&lt;/strong&gt; to convergence&lt;br /&gt;
Using &lt;strong&gt;Gibbs sampling&lt;/strong&gt; as the transition operator&lt;/p&gt;

&lt;p&gt;Can use &lt;strong&gt;Block Gibbs Sampling&lt;/strong&gt; since visible and hidden units are conditionally independent:&lt;br /&gt;
A step in the Markov Chain is (n-th step):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;h(n+1) ~ sig (Wv(n)+b)
v(n+1) ~ sig (h(n+1)W'+a)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As t-&amp;gt;∞, samples (v(t),h(t)) are guaranteed to be accurate samples of p(v,h)&lt;/p&gt;

&lt;p&gt;2 steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;forward&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; h(1) = sig (Wv(0)+b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reconstruction (backward)&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; v(1) = sig (h(1)W'+a)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-learning-process&quot;&gt;The Learning Process&lt;/h3&gt;

&lt;p&gt;v(0)-v(1) &amp;lt;- the reconstruction err we need to reduce in subsequent steps of the training process&lt;/p&gt;

&lt;p&gt;In the forward pass&lt;br /&gt;
p(h(1)|v(0);W) - the probability of output h(1) given the input v(0) and the weights W&lt;/p&gt;

&lt;p&gt;In the backward pass&lt;br /&gt;
p(v(1)|h(1);W)&lt;/p&gt;

&lt;p&gt;Note: W in the forward/backward passes are the same&lt;/p&gt;

&lt;p&gt;p(h|v;W) and p(v|h;W) lead to a joint distribution of inputs and the activations p(v,h)&lt;/p&gt;

&lt;p&gt;Suppose the energy is given by E(v,h)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;E(v,h) = - Σ a_i*v_i - Σ b_j*h_j - Σ v_i*h_j*w_ij
           i           j           i,j
           visible     hidden

v_i,h_j - the binary states of the visible unit i and hidden unit j
a_i,b_j - their biases
w_ij - the weights between them  

p(v) = 1/Z Σ exp(-E(v,h))
           h

Z = Σ   exp(-E(v,h))
   v,h

       h
       Σ exp(-E(v,h))
p(v) = --------------
       Σ exp(-E(v,h))
      v,h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The log-likelihood gradient / the derivative of the log probability of a training vector w.r.t a weight is surprisingly simple:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;∂log p(v)
--------- = &amp;lt;v_i*h_j&amp;gt;     - &amp;lt;v_i*h_j&amp;gt;
  ∂w_ij             |data           |model

&amp;lt;.&amp;gt; - expectations

Δw_ij = α (&amp;lt;v_i*h_j&amp;gt;     - &amp;lt;v_i*h_j&amp;gt;)
                   |data           |model

α - learning rate  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it’s easy to get an unbiased sample of &amp;lt;v_i*h_j&amp;gt;_data, because no direct connections between hidden units&lt;/li&gt;
  &lt;li&gt;it’s difficult to get an unbiased sample of &amp;lt;v_i*h_j&amp;gt;_model, because it requires us to run a Markov chain until the stationary distribution is reached (which means the energy of the distribution is minimized - equilibrium!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instead of doing this, we can perform &lt;strong&gt;Gibbs Sampling&lt;/strong&gt; from the distribution&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gibbs Sampling&lt;/strong&gt; - an MCMC algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Gibbs chain&lt;/strong&gt; is initialized with a training sample v(0) and yields the sample v(k) after k steps&lt;br /&gt;
Each step t consists of sampling h(t) from p(h|v(t)) and sampling v(t+1) from p(v|h(t)) subsequently&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;v(0) -&amp;gt; h(t) ~ p(h|v(t)) -&amp;gt; v(t+1) ~ p(v|h(t)) -&amp;gt; v(k)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The learning rule becomes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Δw_ij = α (&amp;lt;v_i*h_j&amp;gt;     - &amp;lt;v_i*h_j&amp;gt;)
                   |data           |recons
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The learning works well even though it is only crudely approximating the gradient of the log probability of the training data.&lt;br /&gt;
The learning rule is much more closely approximating the gradient of another objective function called the &lt;strong&gt;Contrastive Divergence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contrastive Divergence (CD-k)&lt;/strong&gt; - the difference between two KL divergence:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                          ∂E(v_k,h)              ∂E(v_k,h)
CD(W,v(0)) = - Σ p(h|v_k) --------- + Σ p(h|v_k) ---------
  |k           h              ∂W      h              ∂W
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the second term is obtained after each k steps of Gibbs Sampling&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The general intuition is that if parameter updates are small enough compared to the mixing rate of the chain, the Markov Chain should be able to ‘catch up’ to changes in the model.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;rbm-implementation&quot;&gt;RBM Implementation&lt;/h3&gt;

&lt;p&gt;Given MNIST binary data (60000x28x28 with only {0,1} values)&lt;br /&gt;
Train an RBM model with weights W, and bias a,b and generate learned samples&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;variable    dimension
 v∈{0,1}      784x1
 h∈{0,1}      100x1
    W         784x100
    a         784x1
    b         100x1

E(v,h) = - a'v    -    b'h     -     v'Wh
           / \         / \           /  \
      1x784x784x1    1x100x100x1   1x784x784x100x100x1

p(h=1|v) = sig(W'v+b)      
               / \        
            100x784x784x1   
            100x1+100x1

p(v=1|h) = sig(Wh+a)
               /  \
             784x100x100x1
             784x1+784x1

h,v need to be sampled from p(h=1|v),p(v=1|h):
h ~ p(h=1|v)
v ~ p(v=1|h)

gradient update rule:
W += α(v0h0'-v1h1') 784x100
a += α(v0-v1) 784x1
b += α(h0-h1) 100x1

sig - element-wise sigmoid function
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RBM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hid&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_energy_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#p(h=1|v) = sig(W'v+b)  100x1=100x784x784x1+100x1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h_dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#p(v=1|h) = sig(Wh+a)  784x1=784x100x100x1+784*1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v_dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;true_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sampled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampled&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;energy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#E(v,h) = - a'v - b'h - v'Wh&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#np.random.shuffle(data)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;energy_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#784x1&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;h_sampled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;v_sampled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;h_recon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;#W += alpha(v0h0'-v1h1') 784x100&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_recon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_sampled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sampled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_recon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;energy_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;energy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_recon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_energy_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;energy_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ep:'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' data_idx:'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' avg_energy:'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;energy_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gibbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_init&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;v_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#784x1&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#784x1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h_dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;v_dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#training&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mnist_bin.npy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#60000x28x28&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rbm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RBM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#n_hid 100, n_vis 784&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rbm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_energy_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'avg_energy.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#sampling&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#mnist figure 0~10&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mnist_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gibbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rbm_result.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;350&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results of hyperparameters:&lt;br /&gt;
episode length: 10000&lt;br /&gt;
sample size: 200x28x28   &lt;br /&gt;
alpha: 0.1&lt;br /&gt;
Gibbs iterations: 1000&lt;/p&gt;

&lt;p&gt;Average Energy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/rbm_energy.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Original mnist figure samples &amp;amp; Reconstructed samples:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/rbm_example.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt; &lt;img src=&quot;https://ha5ha6.github.io/judy_blog/assets/images/rbm_result_10000_200_0.1.png&quot; alt=&quot;&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Summary&lt;/strong&gt;:&lt;br /&gt;
Training process:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  forward
  training
v --------&amp;gt; h
|   W,a,b   |
v &amp;lt;-------- h
  backward
  training
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Reconstruction using Gibbs sampling:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input: v_in, i.e. mnist figure '0'~'9'
output: v_out, reconstruction of v_in

     forward(v,w,b)   backward(h,w,a)             
v_in -------------&amp;gt; h --------------&amp;gt; v -&amp;gt; ...... -&amp;gt; v_out  
                                           n_iter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;dimensionality reduction&lt;/li&gt;
  &lt;li&gt;classification&lt;/li&gt;
  &lt;li&gt;regression&lt;/li&gt;
  &lt;li&gt;collaborative filtering&lt;/li&gt;
  &lt;li&gt;feature learning&lt;/li&gt;
  &lt;li&gt;topic modeling&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/h3&gt;

&lt;p&gt;only use inputs X for learning&lt;/p&gt;

&lt;p&gt;(+) automatically extract meaningful features for the data  &lt;br /&gt;
(+) leverage the availability of unlabeled data&lt;br /&gt;
(+) add a data-dependent regularizer to training (-log p(X))&lt;/p&gt;

&lt;p&gt;Algorithms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RBM&lt;/li&gt;
  &lt;li&gt;autoencoder&lt;/li&gt;
  &lt;li&gt;sparse coding model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/FengZiYjun/Restricted-Boltzmann-Machine&quot;&gt;RBM+mnist - github FengZiYjun&lt;/a&gt;
&lt;a href=&quot;http://deeplearning.net/tutorial/rbm.html#implementation&quot;&gt;DeepLearning 0.1 documentation&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/restricted-boltzmann-machines-simplified-eab1e5878976&quot;&gt;Restricted Boltzmann Machine - Simplified&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bipartite_graph&quot;&gt;Bipartite graph - wiki&lt;/a&gt;&lt;br /&gt;
[1] LeCun, Yann, et al. “A tutorial on energy-based learning.” Predicting structured data 1.0 (2006).&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="EBM" /><summary type="html">Energy-based Model</summary></entry><entry><title type="html">Energy-based Model</title><link href="http://localhost:4000/ml-basic/2020/02/05/energybasedmodel.html" rel="alternate" type="text/html" title="Energy-based Model" /><published>2020-02-05T14:59:25+09:00</published><updated>2020-02-05T14:59:25+09:00</updated><id>http://localhost:4000/ml-basic/2020/02/05/energybasedmodel</id><content type="html" xml:base="http://localhost:4000/ml-basic/2020/02/05/energybasedmodel.html">&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The main purpose of statistical modeling and machine learning is to encode dependencies between variables. By capturing those dependencies, a model can be used to answer questions about the values of unknown variables given the values of known variables.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Energy-Based Models&lt;/strong&gt; (EBMs) capture dependencies by associating a scalar energy (a measure of compatibility) to each configuration of the variables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; consists in setting the value of observed variables and finding values of the remaining variables that minimize the energy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning&lt;/strong&gt; consists in finding an energy function that associates low energies to correct values of the remaining variables, and higher energies to incorrect values.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;loss functional&lt;/strong&gt;, minimized during learning, is used to measure the quality of the available energy functions.&lt;/p&gt;

&lt;p&gt;With this common &lt;strong&gt;inference/learning framework&lt;/strong&gt;, the wide choice of &lt;strong&gt;energy functions and loss functionals&lt;/strong&gt; allows for the design of many types of &lt;strong&gt;statistical models&lt;/strong&gt;, both &lt;strong&gt;probabilistic and non-probabilistic&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;non-probabilistic training of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;graphical models&lt;/li&gt;
  &lt;li&gt;other structured models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;probabilistic estimation for&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;prediction&lt;/li&gt;
  &lt;li&gt;classification&lt;/li&gt;
  &lt;li&gt;decision-making&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Because&lt;/strong&gt; there is no requirement for proper normalization, &lt;strong&gt;energy-based approaches avoid the problems associated with estimating the normalization constant in probabilistic models&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X (observed variables, i.e. image pixels)
Y (variables to be predicted, i.e. classification labels of 'Human','Animal','Car', ...)

X,Y -&amp;gt; E(Y,X) (energy function)

          Energy (the lower the better)
Human     ||||||
Animal*   |||
Car       |||||||||
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;energy-based-inference&quot;&gt;Energy-Based Inference&lt;/h3&gt;

&lt;p&gt;Let us consider a model with two sets of variables, X and Y as above example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Energy function E(Y,X)&lt;/strong&gt; measures the “goodness”/”badness” of each possible configuration of X and Y, and the output number can be interpreted as the degree of &lt;strong&gt;compatibility&lt;/strong&gt; between the values of X and Y.&lt;/p&gt;

&lt;p&gt;Energy function also can be called:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;contrast functions&lt;/li&gt;
  &lt;li&gt;value functions&lt;/li&gt;
  &lt;li&gt;negative log-likelihood functions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Distinction:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;energy function - is minimized by the inference process&lt;/li&gt;
  &lt;li&gt;loss functional - is minimized by the learning process&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Normally, the input X is given, and the model produces the answer Y&lt;br /&gt;
Precisely, the model must produce the value Y*, from a set Y_, for which E(Y,X) is the smallest&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Y* = argmin E(Y,X)
      Y∈Y_
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If Y_ is small, we can simply compute E(Y,X) for all possible Y∈Y_, and pick the smallest &lt;br /&gt;
Sometime Y_ can be too large to make exhaustive search practical&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inference procedure&lt;/strong&gt; is employed to find the Y that minimizes E(Y,X), an approximate result, which may or may not be the global minimum of E(Y,X)&lt;/p&gt;

&lt;p&gt;The quality of inference procedure depends on the internal structure of the model, i.e.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gradient-based optimization &amp;lt;- if Y_ is continuous and E(Y,X) is smooth and well-behaved with respect to Y&lt;/li&gt;
  &lt;li&gt;factor graphs inference, i.e. min-sum &amp;lt;- if Y is a collection of discrete variables and the energy function can be expressed as a factor graph&lt;/li&gt;
  &lt;li&gt;dynamic programming, i.e. Viterbi algorithm or A* &amp;lt;- if each element of Y_ can be represented as a path in a weighted directed acyclic graph, then the energy for a particular Y is the sum of values on the edges and nodes along a particular path&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;often occurs in sequence labeling problems i.e.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sequence recognition&lt;/li&gt;
  &lt;li&gt;handwriting recognition&lt;/li&gt;
  &lt;li&gt;nlp&lt;/li&gt;
  &lt;li&gt;biological sequence analysis (i.e. gene finding, protein folding prediction, etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;other optimization procedures:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;continuous optimization, i.e. linear programming, quadratic programming&lt;/li&gt;
  &lt;li&gt;non-linear or discrete optimization, i.e. simulated annealing, graph cuts, graph matching&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;when exact optimization is impractical, can resort to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;approximate methods, i.e. methods use surrogate energy functions (i.e. variational methods)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What questions can a model answer?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;prediction, classification, decision-making - Which value of Y is most compatible with this X?&lt;/li&gt;
  &lt;li&gt;ranking - Is Y1 or Y2 more compatible with this X?&lt;/li&gt;
  &lt;li&gt;detection - Is this value of Y compatible with X?&lt;/li&gt;
  &lt;li&gt;conditional density estimation - What is the conditional probability distribution over Y_ given X?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Situations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;X as a high-dimensional variable (image), Y as a discrete variable (label)&lt;/li&gt;
  &lt;li&gt;the converse case of above, i.e. image restoration, computer graphics, speech and language production&lt;/li&gt;
  &lt;li&gt;both X and Y are high-dimensional&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decision-making-vs-probabilistic-modeling&quot;&gt;Decision Making VS Probabilistic Modeling&lt;/h3&gt;

&lt;p&gt;In Decision Making:&lt;/p&gt;

&lt;p&gt;Problem:&lt;br /&gt;
Because energies are uncalibrated (measured in arbitrary units), combining two separately trained energy-based models is not straightforward&lt;/p&gt;

&lt;p&gt;Solution:&lt;br /&gt;
the only consistent way to combine energies involves turning the collection of energies for all possible outputs into a normalized probability distribution&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gibbs distribution&lt;/strong&gt; - the simplest and most common method for turning a collection of arbitrary energies into a collection of numbers between 0 and 1 whose sum/integral is 1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;           exp(-βE(Y,X))
P(Y|X) = ------------------
          ∫ exp(-βE(y,X))
         y∈Y_

β - an arbitrary positive constant akin to an inverse temperature
∫ exp(-βE(y,X)) - partition function  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Whether the numbers obtained this way are good probability estimates doesn’t depend on how energies are turned into probabilities, but on how E(Y,X) is estimated from data.&lt;/li&gt;
  &lt;li&gt;The above transformation of energies into probabilities is only possible if the integral ∫ exp(-βE(y,X)), y∈Y_ converges.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some situations that computing the partition function is intractable or outright impossible:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;when Y_ has high cardinality&lt;/li&gt;
  &lt;li&gt;when Y_ is a high dimensional variable and the integral has no analytical solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Hence&lt;/strong&gt;, probabilistic modeling comes with a high price, and should be avoided when the application does not require it&lt;/p&gt;

&lt;h3 id=&quot;energy-based-training-architecture-and-loss-function&quot;&gt;Energy-Based Training: Architecture and Loss Function&lt;/h3&gt;

&lt;p&gt;Training an EBM consists in &lt;strong&gt;finding an energy function that produces the best Y for any X&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The search for the best energy function is performed within a family of energy functions ε&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ε = {E(W,Y,X): W∈W_}  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;Architecture&lt;/strong&gt; of the EBM is the internal structure of the parameterized energy function E(W,Y,X).&lt;/p&gt;

&lt;p&gt;When X and Y are real vectors, ε could be&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;as simple as linear combination of basis functions (as in the case of kernel methods)&lt;/li&gt;
  &lt;li&gt;a set of neural net architectures and weight values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One &lt;strong&gt;advantage&lt;/strong&gt; of the EBMs is:&lt;br /&gt;
it puts very little restrictions on the nature of ε&lt;/p&gt;

&lt;p&gt;To train the model for prediction, classification, decision-making:&lt;/p&gt;

&lt;p&gt;Given a set of trainning samples, S={(X_i,Y_i):i=1…P}&lt;br /&gt;
X_i: the input for i-th&lt;br /&gt;
Y_i: the corresponding desired answer&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss functional&lt;/strong&gt; (i.e. a function of function denoted L(E,S)) - In order to find the best energy function in the family ε, we need a way to access the quality of any particular energy function, based on &lt;strong&gt;the training set and our prior knowledge about the task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For simplicity, we denote it L(W,S) and call it the &lt;strong&gt;loss function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(Loss function is an evaluation function of energy function)&lt;/p&gt;

&lt;p&gt;The learning problem is imple to find the W that minimizes the loss:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;W* = min L(W,S)
     W∈W_
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For most cases, the loss functional is defined as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;              P
L(E,S) = 1/P  Σ  { l(Y_i, E(W,Y_,X_i)) + R(W) }
             i=1

l(Y_i, E(W,Y_,X_i)) - per-sample loss functional  
R(W) - regularizer, can be used to embed prior knowledge about which energy functions in our family are preferable to others  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Per-sample loss functional&lt;/strong&gt; should assign a low loss to well-behaved energy functions (functions that give the lowest energy to the correct answer and higher energy to all other answers.&lt;/p&gt;

&lt;p&gt;Suppose:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Y_i - the correct answer&lt;/li&gt;
  &lt;li&gt;Y_i* - the answer produced by the model, i.e. the answer with the lowest energy&lt;/li&gt;
  &lt;li&gt;Y_i’ - the most offending incorrect answer, i.e. the answer that has the lowest energy among all the incorrect answers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With a properly designed loss function, the &lt;strong&gt;learning process&lt;/strong&gt; should have the effect of “pushing down” on E(W,Y_i,X_i) and “pulling up” on the incorrect energies on E(W,Y_i’,X_i)&lt;/p&gt;

&lt;p&gt;Different loss functions do this in different ways.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a training set S, building and training an EBM involves designing four components:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the architecture: the internal structure of E(W,Y,X)&lt;/li&gt;
  &lt;li&gt;the inference algorithm: the method for finding a value of Y that minimizes E(W,Y,X) for any given X&lt;/li&gt;
  &lt;li&gt;the loss function: L(W,S) measures the quality of an energy function using the training set&lt;/li&gt;
  &lt;li&gt;the learning algorithm: the method for finding a W that minimizes the loss functional over the family of energy functions ε, given the training set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Choosing the combinations of architecture and loss functions that can learn effectively and efficiently is critical to the energy-based approach!!&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;loss-functions&quot;&gt;Loss Functions&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Energy Loss&lt;/strong&gt;:&lt;br /&gt;
For a training sample (X_i,Y_i), the per-sample loss is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L ( Y_i, E(W,Y_,X_i) ) = E( W,Y_i,X_i )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This loss function:&lt;br /&gt;
(-) cannot be used to train most architectures: while this loss will push down on the energy of the desired answer, it will not pull up on any other energy&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Collapsed Solution&lt;/em&gt; - the energy is constant and equal to zero&lt;/p&gt;

&lt;p&gt;The energy loss will only work with architectures that are designed in such a way that pushing down on E(W,Y_i,X_i) will automatically make the energies of the other answers larger&lt;br /&gt;
i.e. E ( W,Y_i,X_i )=|| Y_i - G(W,X_i) ||^2 &amp;lt;- regression with MSE with G being the regression function&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generalized Perceptron Loss&lt;/strong&gt;:&lt;br /&gt;
For a training sample (X_i,Y_i) is defined as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L ( Y_i, E(W,Y_,X_i) ) = E( W,Y_i,X_i ) - min E( W,Y,X_i )
                                          Y∈Y_
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This loss function:&lt;br /&gt;
(+) always positive&lt;br /&gt;
(+) minimizing this loss has the effect of pushing down on E(W,Y_i,X_i), while pulling up on the energy of the answer produced by the model  &lt;br /&gt;
(-) there is no mechanism for creating an energy gap between the correct answer and the incorrect ones  &lt;br /&gt;
(-) hence, may produce flat energy surfaces&lt;/p&gt;

&lt;p&gt;Can apply on models with structured outputs such as&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;handwriting recognition [LeCun et al 1998a]&lt;/li&gt;
  &lt;li&gt;parts of speech tagging [Collins, 2002]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generalized Margin Loss&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the hinge loss&lt;/li&gt;
  &lt;li&gt;log loss&lt;/li&gt;
  &lt;li&gt;LVQ2 loss&lt;/li&gt;
  &lt;li&gt;minimum classification error loss&lt;/li&gt;
  &lt;li&gt;square-square loss&lt;/li&gt;
  &lt;li&gt;square-exponential loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All use some form of margin to create an energy gap between the correct answers and the incorrect answers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;: Let Y be a discrete variable.&lt;br /&gt;
For a training sample (X_i,Y_i), the most offending incorrect answer Y_i’ is the answer that has the lowest energy among all answers that are incorrect:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Y_i' =   argmin       E(W,Y,X_i)
      Y∈Y_ &amp;amp; Y!=Y_i
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Definition 2&lt;/strong&gt;: Let Y be a continuous variable.&lt;br /&gt;
For a training sample (X_i,Y_i), the most offending incorrect answer Y_i’ is the answer that has the lowest energy among all answers that are at least ε away from the correct answer:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Y_i' =     argmin           E(W,Y,X_i)
      Y∈Y_, ||Y-Y_i||&amp;gt;ε
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;generalized margin loss&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L (W,Y_i,X_i) = Qm ( E(W,Y_i,X_i), E(W,Y_i',X_i) )  

m - margin, a positive parameter
Qm - a convex function whose gradient has a positive dot product with the vector [1,-1] in the region where E(W,Y_i,X_i)+m&amp;gt;E(W,Y_i',X_i)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In other words, the loss surface is slanted toward low values of E(W,Y_i,X_i) and high values of E(W,Y_i’,X_i) wherever E(W,Y_i,X_i) is not smaller than E(W,Y_i’,X_i) by at least m.&lt;/p&gt;

&lt;p&gt;Two special cases:&lt;br /&gt;
&lt;strong&gt;Hinge Loss&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L (W,Y_i,X_i) = max ( 0, m + E(W,Y_i,X_i) - E(W,Y_i',X_i) )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Log Loss&lt;/strong&gt;: a “soft” version of the hinge loss with an infinite margin&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L (W,Y_i,X_i) = log ( 1 + exp ( E(W,Y_i,X_i)-E(W,Y_i',X_i) ) )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;simple-architectures&quot;&gt;Simple Architectures&lt;/h3&gt;

&lt;p&gt;classification and regression -&amp;gt; EBMs&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;E(W,Y,X) = 1/2 ||Gw(X)-Y||^2   

        N
Gw(X) = Σ wkφk(X) = W' Φ(X)    
       k=1

                P                     P
L(W,S) = 1/P  Σ E(W,Y_i,X_i) = 1/2P Σ ||Gw(X_i)-Y_i||^2
               i=1                   i=1

                   P
W* = argmin [ 1/2P Σ || W'Φ(X_i) - Y_i ||^2]
                  i=1

L(W,S) - standard regression with mean-squared error
Gw - regression function
φk(X) - N features
φk(X)=K(X,X_k), k=1..P, K - kernel function  
wk - N parameter vector W
Y - the variable to be predicted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] LeCun, Yann, et al. “A tutorial on energy-based learning.” Predicting structured data 1.0 (2006).&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="EBM" /><summary type="html">Definition</summary></entry><entry><title type="html">Modular RL</title><link href="http://localhost:4000/rl-basic/2020/01/16/modulerrl.html" rel="alternate" type="text/html" title="Modular RL" /><published>2020-01-16T12:02:25+09:00</published><updated>2020-01-16T12:02:25+09:00</updated><id>http://localhost:4000/rl-basic/2020/01/16/modulerrl</id><content type="html" xml:base="http://localhost:4000/rl-basic/2020/01/16/modulerrl.html">&lt;h3 id=&quot;modular-deep-reinforcement-learning-with-temporal-logic-specification&quot;&gt;Modular Deep Reinforcement Learning with Temporal Logic Specification&lt;/h3&gt;

&lt;p&gt;a modular Deep Deterministic Policy Gradient (DDPG) architecture is proposed to generate a low-level control policy&lt;/p&gt;

&lt;p&gt;Deep reinforcement learning is an emerging paradigm for autonomous solving of decision-making tasks in &lt;strong&gt;complex and unknown environments&lt;/strong&gt;. However, tasks featuring extremely &lt;strong&gt;delayed rewards&lt;/strong&gt; are often difficult, if at all possible, to solve with monolithic learning in Reinforcement Learning (RL). A well-known example is the Atari game Montezuma’s Revenge in which deep RL methods such as (Mnih et al. 2015) failed to score even once.&lt;/p&gt;

&lt;p&gt;Despite their generality, it is not fair to compare deep RL methods with how humans learn these problems, since &lt;strong&gt;humans already have prior knowledge and associations regarding elements and their corresponding function&lt;/strong&gt;, e.g. “keys open doors” in Montezuma’s Revenge. These simple yet &lt;strong&gt;critical temporal high-level associations&lt;/strong&gt; in Montezuma’s Revenge and a large number of real world complex problems, can lift deep RL initial knowledge about the problem to efficiently find the global optimal policy, while &lt;strong&gt;avoiding an exhaustive unnecessary exploration in the beginning&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Hierarchical RL -&amp;gt; options&lt;/p&gt;

&lt;p&gt;LTL - Linear Temporal Logic&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;used to encode the structure of the high-level mission task and to automatically shape the reward function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Refs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[Precup 2001] Precup, D. 2001. Temporal abstraction in reinforcement learning. Ph.D. Dissertation, University of Massachusetts Amherst.&lt;br /&gt;
[Kearns and Singh 2002] Kearns, M., and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine learning 49(2-3):209–232.&lt;br /&gt;
[Daniel, Neumann, and Peters 2012] Daniel, C.; Neumann, G.; and Peters, J. 2012. Hierarchical relative entropy policy search. In Artificial Intelligence and Statistics, 273–281.&lt;br /&gt;
[Kulkarni et al. 2016] Kulkarni, T. D.; Narasimhan, K.; Saeedi, A.; and Tenenbaum, J. 2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, 3675–3683.&lt;br /&gt;
[Vezhnevets et al. 2016] Vezhnevets, A.; Mnih, V.; Osindero, S.; Graves, A.; Vinyals, O.; Agapiou, J.; et al. 2016. Strategic attentive writer for learning macro-actions. In Advances in neural information processing systems, 3486–3494.&lt;br /&gt;
[Andreas, Klein, and Levine 2017] Andreas, J.; Klein, D.; and Levine, S. 2017. Modular multitask reinforcement learning with policy sketches. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, 166–175.&lt;/p&gt;

&lt;h3 id=&quot;modular-reinforcement-learning-an-application-to-a-real-robot-task&quot;&gt;Modular Reinforcement Learning An Application to a Real Robot Task&lt;/h3&gt;

&lt;p&gt;The key idea is to break up the problem into subtasks and design controllers for each of the subtasks. Then operating conditions are attached to the controllers (together the controllers and their operating conditions which are called modules) and possible additional features are designed to facilitate observability.&lt;/p&gt;

&lt;p&gt;A new discrete time-counter is introduced at the “module-level” that clicks only when a change in the value of one of the features is observed.&lt;/p&gt;

&lt;p&gt;The learnt switching strategy performed equally well as a handcrafted version.&lt;/p&gt;

&lt;p&gt;RL algorithms are based on modifications of the two basic dynamic-programming algorithms used to solve MDPs namely the value- and policy-iteration algorithms.&lt;/p&gt;

&lt;p&gt;Problem: Partial Observability&lt;/p&gt;

&lt;p&gt;In this article an attempt is made to show that RL can be applied to learn real life tasks &lt;strong&gt;when a priori knowledge is combined in some suitable way.&lt;/strong&gt; The key to our proposed method lies in the use of high-level modules along with a specification of the operating conditions for the modules and other features to transform the task into a finite-state and action completely-observable task.&lt;/p&gt;

&lt;p&gt;Bellman equations can be solved by various dynamic programming methods such as the value- or policy-iteration methods.&lt;/p&gt;

&lt;p&gt;There are two possible ways to learn the optimal value-function.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;to estimate the model i.e. the transition probabilities and immediate costs&lt;/li&gt;
  &lt;li&gt;to estimate the optimal action-values directly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A switching function S maps featrue-vectors to the indices of modules: S(f)=i&lt;/p&gt;

&lt;p&gt;RL can&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;find the best switching function assuming that at least two proper switching functions exist&lt;/li&gt;
  &lt;li&gt;decide empirically whether a valid switching controller exists at all&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The work of Connell and Mahadevan complements the works in that they set-up subtasks to be learned by RL and fixed the switching controller.&lt;br /&gt;
[S Mahadevan and J Connell Automatic programming of behavior-based robots using reinforcement learning. Artificial Intelligence]&lt;/p&gt;

&lt;p&gt;Asada et al. describe a goal-shooting problem in which a mobile robot shot a goal while avoiding another robot [Uchibe 1996]. The robot learned two behaviors separately: the “shot” and “avoid” behaviors. Then the two behaviors were synthetized by a handcrafted rule and later this rule was refined via RL. The learnt action-values of the two behaviors were reused in the learning process while the combination of rules took place at the level of state variables.&lt;/p&gt;

&lt;p&gt;[Uchibe 1996] Behavior coordination for a mobile robot using modular reinforcement learning.&lt;/p&gt;

&lt;h3 id=&quot;composable-modular-reinforcement-learning&quot;&gt;Composable Modular Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;Truly modular RL would support not only decomposition into modules, but composability of separately written modules in new modular RL agents.&lt;/p&gt;

&lt;p&gt;However, the performance of MRL agts that arbitrate module preferences using additive reward schemes degrades when the modules have incomparable reward scales. The performance degradation means that separately written modules cannot be composed in new modular RL agents as-is - they may need to be modified to align their reward scales.&lt;/p&gt;

&lt;p&gt;The problem is solved with a Q-learning based command arbitration algorithm and demonstrate that it does not exhibit the same performance degradation as existing approaches to MRL.&lt;/p&gt;

&lt;p&gt;Decomposition is an important tool for dealing with the larger state spaces likely to be encountered in real-world problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical RL&lt;/strong&gt; decomposes RL problems &lt;strong&gt;temporally&lt;/strong&gt;, modeling intermediate tasks as higher-level actions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modular RL&lt;/strong&gt; decomposes the original problem &lt;strong&gt;concurrently&lt;/strong&gt;, modeling an agent as a set of concurrently running RL modules.&lt;/p&gt;

&lt;p&gt;MRL has been used primarily &lt;strong&gt;to model multi-goal problems&lt;/strong&gt; and &lt;strong&gt;to deal with large state spaces.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation&quot;&gt;Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation&lt;/h3&gt;

&lt;p&gt;Learning goal-directed behavior with sparse feedback from complex environments is a fundamental challenge for artificial intelligence.&lt;/p&gt;

&lt;p&gt;Learning in this setting requires the agent to &lt;strong&gt;represent knowledge at multiple levels of spatio-temporal abstractions&lt;/strong&gt; and to explore the environment efficiently.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;represent knowledge at multiple levels of spatio-temporal abstractions -&amp;gt; non-linear function approximators + RL&lt;/li&gt;
  &lt;li&gt;explore with sparse feedback -&amp;gt; still remains a major challenge&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Exploration methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Boltzmann exploration [31]&lt;/li&gt;
  &lt;li&gt;Thomson sampling [19]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this work, we propose a framework that integrates &lt;strong&gt;deep reinforcement learning with hierarchical action-value functions&lt;/strong&gt; (h-DQN), where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the top-level module learns a policy over options (subgoals) and&lt;/li&gt;
  &lt;li&gt;the bottom-level module learns policies to accomplish the objective of each option&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model takes decisions over two levels of hierarchy –&lt;br /&gt;
(a) a top level module (meta-controller) takes in the state and picks a new goal, and&lt;br /&gt;
(b) a lower-level module (con- troller) uses both the state and the chosen goal to select actions either until the goal is reached or the episode terminates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value functions&lt;/strong&gt; V (s) are central to RL, and they cache the utility of any state s in achieving the agent’s overall objective.&lt;br /&gt;
Recently, value functions have also been generalized as V (s, g) in order to represent the utility of state s for achieving a given goal g ∈ G [33, 21].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Abstraction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hierarchical reinforcement learning [2] - Barto&lt;br /&gt;
&lt;strong&gt;“Options” framework&lt;/strong&gt; [34] - Sutton - involves abstractions over the space of actions&lt;/p&gt;

&lt;p&gt;At each step, the agent chooses either a one step “primitive” action or a “multi-step” action poilicy (option).&lt;br /&gt;
Each option defines a policy over actions and can be terminated according to a stochastic function β.&lt;/p&gt;

&lt;p&gt;MDP -&amp;gt; semi-MDP&lt;/p&gt;

&lt;p&gt;recent proposed methods about learning options in real-time&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;by using varying reward functions [35]&lt;/li&gt;
  &lt;li&gt;by composing existing options [28]&lt;/li&gt;
  &lt;li&gt;value functions considering goals along with states [21]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;MAXQ framework&lt;/strong&gt; decomposed the value function of an MDP into combinations of value functions of smaller constituent MDPs [6]&lt;/p&gt;

&lt;p&gt;[31] B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.&lt;br /&gt;
[19] I.Osband,C.Blundell,A.Pritzel,andB.VanRoy.Deepexploration via bootstrapped dqn.arXivpreprint arXiv:1602.04621, 2016. &lt;br /&gt;
[2] A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(4):341–379, 2003.&lt;br /&gt;
[34] R.S.Sutton,D.Precup,andS.Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181–211, 1999.&lt;br /&gt;
[35] C. Szepesvari, R. S. Sutton, J. Modayil, S. Bhatnagar, et al. Universal option models. In Advances in Neural Information Processing Systems, pages 990–998, 2014.&lt;br /&gt;
[28] J. Sorg and S. Singh. Linear options. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems, pages 31–38, Richland, SC, 2010.&lt;br /&gt;
[21] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1312–1320, 2015.&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-reinforcement-learning-learning-sub-goals-and-state-abstraction&quot;&gt;Hierarchical Reinforcement Learning: Learning sub-goals and state-abstraction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical RL&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Motivation - the curse of dimensionality&lt;br /&gt;
problem caused by the exponential growth of parameters to be learned, associated with adding extra variables to a representation of a state. Applying RL with a very large action and state space tunred to be an impossible task.&lt;/p&gt;

&lt;p&gt;HRL introduces various forms of &lt;strong&gt;abstraction and problem hierarchization&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchization&lt;/strong&gt; divides the main problem in sub-problems that can be solved using regular RL. Each sub-problem has its own sub-goal. The sequential resolution of serveral sub-goals takes us to the solution of the main problem.&lt;/p&gt;

&lt;p&gt;Hierarchical Abstract Machines - [Parr and Russell 1998]
The options framework - [Sutton 1999]&lt;/p&gt;

&lt;p&gt;The goal of HRL:&lt;br /&gt;
discovering and exploiting hierarchical structure within a MDP&lt;/p&gt;

&lt;p&gt;Given an MDP, the programmer will be responsible for designing a task hierarchy for a specific problem&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;decomposing the main task into several subtasks that are, in turn, also decomposed until a subtask is reached that is composed only by primitive actions&lt;/li&gt;
  &lt;li&gt;each subtask will learn its own Q function which represents the expected total reward of performing subtask on an initial state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HSMQ - Hierarchical Semi-Markov Q-learning - is seen as a collection of simultaneous, independent Q-Learning problems, no representational decoomposition of the value function&lt;/p&gt;

&lt;p&gt;MAXQ did the value function decomposition [dietterich 2000a] &lt;br /&gt;
MAXQ + state abstractions -&amp;gt; four times efficient &lt;br /&gt;
(-) hand code for the task structure&lt;/p&gt;

&lt;p&gt;HEXQ - automatically tries to decompose and solve a model-free factored MDP [Hengst, 2002]&lt;br /&gt;
automatically discovering state and temporal abstraction, finding appropriate sub-golas in order to construct a hierarchical representation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; is one of the biggest limitations in RL, cuz sub-policies need to be relevant in every new context.&lt;/p&gt;

&lt;p&gt;The perfect solution would be to learn once each sub-task, and then reuse that whenever the skill was needed.&lt;/p&gt;

&lt;p&gt;[Bernhard Hengst. Discovering Hierarchy in Reinforcement Learning with HEXQ. Proceedings of the Nineteenth International Conference on Machine Learning, 2002. URL http://portal.acm.org/citation.cfm?id=645531.656017.]&lt;/p&gt;

&lt;h3 id=&quot;multiple-model-based-reinforcement-learning&quot;&gt;Multiple Model-based Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;MMRL - basic idea:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;decompose a complex task into multiple domains in space and time based on the predictability of the environmental dynamics&lt;/li&gt;
  &lt;li&gt;the system is composed of multiple modules, each of which consists of a state prediction model and a RL controller&lt;/li&gt;
  &lt;li&gt;a “responsibility signal” given by a softmax function of the prediction errors is used to weight the outputs of multiple modules as well as to gate the learning of the prediction models and RL controllers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Real World RL problem: Non-linearity and non-stationarity&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;for non-linear, high-dimensional system, learning is slow&lt;/li&gt;
  &lt;li&gt;for non-stationary, hidden states, perform badly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;modular or hierarchical RL&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Compositional Q-learning - [Singh 1992]&lt;br /&gt;
Feudal reinforcement learning - [Dayan and Hinton 1993]&lt;br /&gt;
Learning policies for partially observable environments: Scaling up - [Littman et al. 1995]&lt;br /&gt;
HQ-learning - [Wiering and Schmidhuber 1998]&lt;br /&gt;
Reinforcement learning with hierarchies of machines - [Parr and Russel 1998]&lt;br /&gt;
Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning - [Sutton 1999]&lt;br /&gt;
Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning - [Morimoto and Doya 2001]&lt;/p&gt;

&lt;p&gt;The basic problem in modular or hierarchical RL is &lt;strong&gt;how to decompose a complex task into simpler subtasks&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;policy-reuse-in-reinforcement-learning-for-modular-agents&quot;&gt;Policy Reuse in Reinforcement Learning for Modular Agents&lt;/h3&gt;

&lt;p&gt;Hierarchical RL addresses continuous environment spaces by using different abstraction levels to learn task-specific partial policies with computable bounds [7]&lt;/p&gt;

&lt;p&gt;For exceedingly large state spaces, HRL has been widely practiced.&lt;br /&gt;
Where multiple objectives and temporal abstractions are adopted to facilitate space explorations.&lt;/p&gt;

&lt;p&gt;i.e. in [11] [12], at each time t and for each state st, a higher level controler chooses the goal gt where G is the set of all possible goals currently available for the controller to choose from.&lt;/p&gt;

&lt;p&gt;RL problems -&amp;gt; modular task, and every problem is actually composed of concurrent sub-problems with matter of abstraction levels. [3]&lt;/p&gt;

&lt;p&gt;Different approaches to hierarchical RL result in variants on this overall approach, choosing different trade-offs in flexibility, training speed, and other properties [25]&lt;/p&gt;

&lt;p&gt;[7] J. Z. Kolter, P. Abbeel, and A. Y. Ng, “Hierarchical apprenticeship learning with application to quadruped locomotion,” in Advances in Neural Information Processing Systems, pp. 769–776, 2008.&lt;br /&gt;
[10] T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine, “Composable deep reinforcement learning for robotic manipulation,” arXiv preprint arXiv:1803.06773, 2018. &lt;br /&gt;
[11] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, “Hier- archical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,” in Advances in neural information processing systems, pp. 3675–3683, 2016.&lt;br /&gt;
[12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in International conference on machine learning, pp. 1928–1937, 2016.&lt;br /&gt;
[13] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of deep visuomotor policies,” The Journal of Machine Learning Research, vol. 17, no. 1, pp. 1334–1373, 2016.&lt;br /&gt;
[14] S. Bhat, C. L. Isbell, and M. Mateas, “On the difficulty of modular reinforcement learning for real-world partial programming,” in Proceedings of the National Conference on Artificial Intelligence, vol. 21, p. 318, Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.&lt;br /&gt;
[3] O. S ̧ims ̧ek, A. P. Wolfe, and A. G. Barto, “Identifying useful subgoals in reinforcement learning by local graph partitioning,” in Proceedings of the 22nd international conference on Machine learning, pp. 816–823, ACM, 2005.&lt;br /&gt;
[25] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman, “Meta learning shared hierarchies,” arXiv preprint arXiv:1710.09767, 2017.&lt;/p&gt;

&lt;h3 id=&quot;on-the-difficulty-of-modular-reinforcement-learning-for-real-world-partial-programming&quot;&gt;On the Difficulty of Modular Reinforcement Learning for Real-World Partial Programming&lt;/h3&gt;

&lt;p&gt;Modular reinforcement learning (MRL) refers to the decomposition of a complex, multi-goal problem into a collection of simultaneously running single-goal learning processes, typically modeled as MDP.   These subagents share an action set but have their own reward signal and state space.&lt;/p&gt;

&lt;p&gt;At each time step, every subagent reports a numerical preference (Q-values) for each available action to &lt;strong&gt;an arbitrator&lt;/strong&gt;, which then selects one of the actions for the agent as a whole to take.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimally combining subagent Q-values in a meaningful way has thus become the focus of recent work.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Arbitration:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;choose the action maximizing average happiness - argmax Sum(Qj)&lt;/li&gt;
  &lt;li&gt;choose the action using winner-take-all - argmax maxj Qj&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Arbitration approaches assume the subagent reward signals are comparable. However, it’s only reasonable for toy problems, not for real-world, multi-goal problems.&lt;/p&gt;

&lt;p&gt;In multi-goal case, not only must the designer properly craft a reward signal for each subagent, she also must &lt;strong&gt;ensure that the reward units are consistent between the subproblems&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Solution: &lt;strong&gt;Social Choice Theory&lt;/strong&gt;&lt;br /&gt;
Reduce the problem of constructing an arbitration function to a variant of Impossibility Theorem for social ordering functions [Arrow 1966] - characterizing MRL as a social welfare problem&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Partial Programming&lt;/strong&gt;&lt;br /&gt;
a designer or programmer specifies only that part of the program known to be correct, allowing a learning system to learn the rest from experience i.e. RL&lt;br /&gt;
One can think partial programming as a way for a designer to inject prior knowledge into a learning agent&lt;/p&gt;

&lt;p&gt;Allowing the programmer to constrain the set of policies considered by hand-authoring a subroutine hierarchy -
[Andre and Russel 2000] - Programmable Reinforcment Learning Agents.&lt;br /&gt;
[Dietterich 1998] - The MAXQ Method for Hierarchical Reinforcement Learning.&lt;/p&gt;

&lt;p&gt;HRL - a temporal dexomposition of goals&lt;br /&gt;
MRL - concurrent subgoal decomposition&lt;/p&gt;

&lt;p&gt;Predator-Food Task:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;avoiding the predator - can assign a large negative reward&lt;/li&gt;
  &lt;li&gt;find food - can assign a large positive reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, how to design the magnitudes of the rewards particularly in relation to each other is difficult! Should maintain some reward consistency.&lt;/p&gt;

&lt;p&gt;Solution: to require the reward signal to be internally consistent, rather than consistent across different subgoals&lt;/p&gt;

&lt;p&gt;Arbitration techniques:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Great Mass Q-Learning [Sprague and Ballard 2003]&lt;/li&gt;
  &lt;li&gt;Top Q-Learning [Humphrys 1996]&lt;/li&gt;
  &lt;li&gt;Negotiated W-Learning []&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multiobjective-reinforcement-learning-a-comprehensive-overview&quot;&gt;Multiobjective Reinforcement Learning: A Comprehensive Overview&lt;/h3&gt;

&lt;p&gt;challenge: to scale up RL to larger and come complex problems - the scaling problem&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a problem has very large or continuous state or action space&lt;/li&gt;
  &lt;li&gt;a problem is best described as a set of hierarchically organized tasks and sub-tasks&lt;/li&gt;
  &lt;li&gt;a problem needs to solve several tasks with different reward simultaneously &amp;lt;- multiobjective RL (MORL)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MORL requires a learning agent to obtain action policies that can optimize two or more objectives at the same time. Each objective has its own associated reward, so the reward is not a scalar but a &lt;strong&gt;vector&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;related objectives, a single objective can be derived by combining all&lt;/li&gt;
  &lt;li&gt;unrelated objectives, each obj can be optimized separately, can find a combined policy to optimize all of them&lt;/li&gt;
  &lt;li&gt;conflicting objectives, (any policy can only max one of the objs), need trade-off&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MORL - combination of multiobj optimization (MOO) + RL to solve the sequential decision making problems with multiple conflicting objs.&lt;/p&gt;

&lt;p&gt;MOO has 2 strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;multi-obj to single-obj strategy, to optimize a scalar value&lt;/li&gt;
  &lt;li&gt;Pareto strategy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multi-obj to single-obj: a scalar value is computed from the multi objs for the utility of an action decision, so the single obj optimization can be used&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weighted sum method [14]&lt;/li&gt;
  &lt;li&gt;Constraint method [15]&lt;/li&gt;
  &lt;li&gt;Sequential method [16]&lt;/li&gt;
  &lt;li&gt;Max-min method [17]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pareto: use the vector-valued utilities, the Pareto optimality concept [25]
The Pareto optimal solutions are defined as noninferior and alternative solutions among the candidate solutions, and they represent the optimal solutions for some possible trade-offs among the multiple conflicting objs.&lt;br /&gt;
Goal is to find Pareto front&lt;/p&gt;

&lt;p&gt;Early approach to solve MDPs is to use DP&lt;br /&gt;
DP computes the optimal policies by estimating the optimal state-action value functions&lt;br /&gt;
(-) DP requires full model info&lt;br /&gt;
(-) large amounts of computation are needed for large state and action spaces&lt;/p&gt;

&lt;p&gt;RL use Monte Carlo + stochastic approximation + function approximation&lt;/p&gt;

&lt;p&gt;Temporal-difference (TD) = Monte Carlo + DP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learn the value function without model like Monte Carlo&lt;/li&gt;
  &lt;li&gt;update the current estimation of value functions partially based on previous learned results&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For discounted reward criteria&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Q learning&lt;/li&gt;
  &lt;li&gt;SARSA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the average reward criteria&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;R learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MORL problem definition:&lt;br /&gt;
MQ^π(s,a)=[Q1^π(s,a),Q2^π(s,a),…,Qn^π(s,a)]^T - vectored state-action value function&lt;br /&gt;
Qi^π - ith obj&lt;/p&gt;

&lt;p&gt;MQ&lt;em&gt;(s,a)=max_π MQ^π(s,a)&lt;br /&gt;
π&lt;/em&gt;(s)=argmax_a MQ*(s,a)&lt;/p&gt;

&lt;p&gt;MORL approaches:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Single-policy approach&lt;/li&gt;
  &lt;li&gt;Multi-policy approach&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Single-policy approach: to obtain the best policy which simultaneously satisfies the preferences among the multiple objs&lt;/p&gt;

&lt;p&gt;Naive solution: to design &lt;strong&gt;a synthetic obj function TQ(s,a)&lt;/strong&gt;, which can suitably represent the overall preferences&lt;/p&gt;

&lt;p&gt;Multi-policy approach: to obtain a set of policies that can approximate the Pareto front&lt;/p&gt;

&lt;p&gt;Naive solution: to find policies in the Pareto front by using different synthetic obj functions.&lt;br /&gt;
Obviously, if a set of parameters can be specified in a synthetic obj function, the optimal policy can be learned for this set of parameters.&lt;/p&gt;

&lt;p&gt;MORL Approaches:&lt;/p&gt;

&lt;p&gt;Single-policy:&lt;/p&gt;

&lt;p&gt;A. Weighted Sum Approaches&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Great Mass&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Synthetic objective function: TQ(s,a)=sum Qi(s,a)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GM-Sarsa(0) - avoid the positive bias problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Synthetic objective function: TQ(s,a)=sum wi*Qi(s,a)&lt;/p&gt;

&lt;p&gt;positive bias problem: caused by off-policy RL methods which only use the estimates of greedy actions for learning updates&lt;br /&gt;
GM-Sarsa(0) is expected to have smaller errors between the estimated Q and the true Q, since the updates are based on the actually selected actions rather than the best action determined by the value function&lt;/p&gt;

&lt;p&gt;Linear weighted sum will meet the problem of concave regions&lt;/p&gt;

&lt;p&gt;B. W-Learning Approaches (winner-take-all)&lt;/p&gt;

&lt;p&gt;to ensure the selected action is optimal for at least one obj&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Top-Q&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Synthetic objective function: TQ(s,a)=max_i Qi(s,a)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;W-learning [32]&lt;/li&gt;
  &lt;li&gt;negotiated W-learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;C. AHP Approach&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the Analytic Hierarchy process [34]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the designer’s prior knowledege of the problem, the degree of relative importance between 2 objs can be quantified by L grades, and a scalar value is defined for each grade&lt;/p&gt;

&lt;p&gt;D. Ranking Approach [37]&lt;/p&gt;

&lt;p&gt;threshold values were specified for some objs in order to put the constraints on the objs&lt;/p&gt;

&lt;p&gt;Synthetic objective function: CQi(s,a)=min {Qi(s,a), Ci}&lt;br /&gt;
Ci - the threshold value for obj i&lt;/p&gt;

&lt;p&gt;E. Geometric Approach&lt;/p&gt;

&lt;p&gt;Multi-policy:&lt;/p&gt;

&lt;p&gt;F. Convex Hull Approach&lt;/p&gt;

&lt;p&gt;G. Varying Parameter Approach&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HRL&lt;/strong&gt; - makes use of a divide-and-conquer strategy to solve complex tasks with large state or decision spaces&lt;/p&gt;

&lt;p&gt;MORL requires the learning agent to solve serveral tasks with differnt objs at once&lt;br /&gt;
HRL aimes to solve sequential decision-making problem that can be best described as a set of hierarchically organized tasks and sub-tasks.&lt;/p&gt;

&lt;p&gt;HRL approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HAMs [81]&lt;/li&gt;
  &lt;li&gt;MAXQ [82]&lt;/li&gt;
  &lt;li&gt;Options [83]&lt;/li&gt;
  &lt;li&gt;ALisp [84]&lt;/li&gt;
  &lt;li&gt;using simi-MDP [85]&lt;/li&gt;
  &lt;li&gt;state space partitioned by critical state [86]&lt;/li&gt;
  &lt;li&gt;HAPI, binary-tree state space decomposition [87]&lt;/li&gt;
  &lt;li&gt;HRL + MORL [88]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[14] I.Y.KimandO.L.deWeck,“Adaptive weighted sum method for multiobjective optimization: A new method for Pareto front generation,” Struct. Multidiscipl. Optim., vol. 31, no. 2, pp. 105–116, 2006.&lt;br /&gt;
[15] A. Konaka, D. W. Coitb, and A. E. Smith, “Multi-objective optimiza- tion using genetic algorithms: A tutorial,” Reliab. Eng. Syst. Safety, vol. 91, no. 9, pp. 992–1007, Sep. 2006.&lt;br /&gt;
[16] M. Yoon, Y. Yun, and H. Nakayama, Sequential Approximate Multiobjective Optimization Using Computational Intelligence. Berlin, Germany: Springer, 2009.&lt;br /&gt;
[17] J. G. Lin, “On min-norm and min-max methods of multi-objective optimization,” Math. Program., vol. 103, no. 1, pp. 1–33, 2005.&lt;/p&gt;

&lt;p&gt;[25] P. Vamplew, J. Yearwood, R. Dazeley, and A. Berry, “On the limitations of scalarisation for multi-objective reinforcement learning of Pareto fronts,” in Proc. 21st Aust. Joint Conf. Artif. Intell., vol. 5360. 2008, pp. 372–378.&lt;/p&gt;

&lt;p&gt;[32] M. Humphrys, “Action selection methods using reinforcement learning,” in From Animals to Animats 4, P. Maes, M. Mataric, J.-A. Meyer, J. Pollack, and S. W. Wilson, Eds. Cambridge, MA, USA: MIT Press, 1996, pp. 134–144. &lt;br /&gt;
[34] Y. Zhao, Q. W. Chen, and W. L. Hu, “Multi-objective reinforcement learning algorithm for MOSDMP in unknown environment,” in Proc. 8th World Congr. Int. Control Autom., 2010, pp. 3190–3194.&lt;br /&gt;
[37] Z. Gabor, Z. Kalmar, and C. Szepesvari, “Multi-criteria reinforcement learning,” in Proc. 15th Int. Conf. Mach. Learn., 1998, pp. 197–205.&lt;br /&gt;
[38] P. Geibel, “Reinforcement learning with bounded risk,” in Proc. 18th Int. Conf. Mach. Learn., 2001, pp. 162–169.&lt;/p&gt;

&lt;p&gt;[81] R. Parr and S. Russell, “Reinforcement learning with hierarchies of machines,” in Advances in Neural Information Processing Systems. Cambridge, MA, USA: MIT Press, 1997, pp. 1043–1049.&lt;br /&gt;
[82] T. Dietterich, “Hierarchical reinforcement learning with the MaxQ value function decomposition,” J. Artif. Intell. Res., vol. 13, no. 1, pp. 227–303, Aug. 2000.&lt;br /&gt;
[83] D. Precup and R. Sutton, “Multi-time models for temporally abstract planning,” in Advances in Neural Information Processing Systems. Cambridge, MA, USA: MIT Press, 1998, pp. 1050–1056.&lt;br /&gt;
[84] D. Andre and S. Russell, “State abstraction for programmable reinforcement learning agents,” in Proc. 18th Nat. Conf. Artif. Intell., 2002, pp. 119–125.&lt;br /&gt;
[85] A. G. Barto and S. Mahadevan, “Recent advances in hierarchical rein- forcement learning,” Discrete Event Dyn. Syst. Theory Appl., vol. 13, nos. 1–2, pp. 341–379, 2003.&lt;br /&gt;
[86] Z. Jin, W. Y. Liu, and J. Jin, “Partitioning the state space by critical states,” in Proc. 4th Int. Conf. Bio-Inspired Comput., 2009, pp. 1–7.&lt;br /&gt;
[87] X. Xu, C. Liu, S. Yang, and D. Hu, “Hierarchial approximate policy iteration with binary-tree state space decomposition,” IEEE Trans. Neural Netw., vol. 22, no. 12, pp. 1863–1877, Dec. 2011.&lt;br /&gt;
[88] H. B. He and B. Liu, “A hierarchical learning architecture with multiple-goal representations based on adaptive dynamic programming,” in Proc. Int. Conf. Netw. Sens. Control, 2010, pp. 286–291.&lt;/p&gt;

&lt;h3 id=&quot;a-generalized-algorithm-for-multi-objective-reinforcement-learning-and-policy-adaptation&quot;&gt;A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation&lt;/h3&gt;

&lt;p&gt;MORL deals with learning control policies to simultaneously optimize over several criteria&lt;/p&gt;

&lt;p&gt;The optimal policy in a multi-obj setting depends on the relative preferences among competing criteria&lt;/p&gt;

&lt;p&gt;MORL
(+) reduced dependence on scalar reward design to combine different objs with is both a tedious manual task and can lead to unintended consequences&lt;br /&gt;
(+) dynamic adaptation or transfer to related tasks with different preferences&lt;/p&gt;

&lt;h3 id=&quot;hra&quot;&gt;HRA&lt;/h3&gt;

&lt;p&gt;challenge of RL: to scale methods such that they can be applied to large, real-world problems&lt;/p&gt;

&lt;p&gt;Because the state-space of such problems is typically massive, strong generalization is required to learn a good policy efficiently &amp;lt;- DRL breakthrough&lt;/p&gt;

&lt;p&gt;Generalization properties of DQN is achieved by approximating the optimal value function&lt;/p&gt;

&lt;p&gt;Value function predicts the expected return, conditioned on a state or state-action pair&lt;/p&gt;

&lt;p&gt;The generalization behavior of DQN is achieved by regularization on the model for the optimal value function.&lt;/p&gt;

&lt;p&gt;However, if the optimal value function is complex, then learning an accurate low-dimensional representation can be challenging or impossible.&lt;/p&gt;

&lt;p&gt;When the optimal value function cannot easily be reduced to a low-dimensional representation, we can &lt;strong&gt;apply a complementary form of regularization on the target side&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Propose to replace the optimal value function as target for training with an alternative value function that is easier to learn, but still yields a reasonable - but generally not optimal - policy, when acting greedily with respect to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The key observation behind regularization on the target function is that two very different value functions can result in the same policy when an agent acts greedily with respect to them.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intrinsic motivation&lt;/strong&gt; uses this observation to improve learning in sparse-reward domains&lt;/p&gt;

&lt;p&gt;[Stout et al 2005] - Intrinsically motivated reinforcement learning: A promising framework for developmental robotics&lt;br /&gt;
[Schmidhuber, 2010] - Formal theory of creativity, fun, and intrinsic motivation&lt;/p&gt;

&lt;p&gt;by adding a domain-specific intrinsic reward signal to the reward coming from the env.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reward Decomposition&lt;/strong&gt;*: decompose the reward function into n different reward functions. Each of them is assigned a separate RL agent.&lt;/p&gt;

&lt;p&gt;Same as &lt;strong&gt;Horde architecture&lt;/strong&gt; [Sutton et al 2011] - Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction&lt;/p&gt;

&lt;p&gt;All the agents can learn in parallel on the same sample sequence by using off-policy learning.&lt;/p&gt;

&lt;p&gt;Each agent gives its action-values of the current state to an aggregator, which combines them into a single value for each action. The current action is selected based on the aggregated values.&lt;/p&gt;

&lt;p&gt;Horde architecture:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a large of “demons” that learn in parallel via off-policy learning&lt;/li&gt;
  &lt;li&gt;each demon trains a separate general value function based on its own policy and pseudo-reward function&lt;/li&gt;
  &lt;li&gt;a pseudo-reward can be any feature-based signal that encodes useful info&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Horde architecture is focused son building up &lt;strong&gt;general knowledge about the world&lt;/strong&gt;, encoded via a large number of GVFs.&lt;/p&gt;

&lt;p&gt;UVFA [Schaul et al. 2015] - Universal value function approximators&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;enables generalization across different tasks/goals&lt;/li&gt;
  &lt;li&gt;does not address how to solve a single, complex task&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multi-objective learning [Roijers et al. 2013] - A survey of multi-objective sequential decision-making&lt;/p&gt;

&lt;p&gt;Reward function decomposition:&lt;br /&gt;
[Russell and Zimdar 2003] - Q-decomposition for reinforcement learning agents&lt;br /&gt;
[Sprague and Ballard 2003] - Multiple-goal reinforcement learning with modular sarsa(0)&lt;/p&gt;

&lt;p&gt;HRA and UNREAL [Jaderberg et al 2017] - Reinforcement learning with unsupervised auxiliary tasks&lt;br /&gt;
(same) solve multiple smaller problems in order to tackle a hard one&lt;br /&gt;
(diff) working ways&lt;br /&gt;
(diff) the challenge they address&lt;/p&gt;

&lt;p&gt;UNREAL:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;boosts representation learning in difficult scenarios&lt;/li&gt;
  &lt;li&gt;by using auxiliary tasks to help train the lower-level layers of the nn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HRA:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HRA’s multiple smaller tasks are not unsupervised, they are tasks directly relevant to the main task&lt;/li&gt;
  &lt;li&gt;HRA is agnostic to the type of function approximation used, i.e. dnn or tabular representation&lt;/li&gt;
  &lt;li&gt;useful for domains where having a high-quality representation is not sufficient to solve the task efficiently&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Options”:&lt;br /&gt;
[Sutton et al 1999] - Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning&lt;br /&gt;
[Bacon et al 2017] - The option-critic architecture&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;options&lt;/strong&gt; are temporally-extended actions that can be trained in parallel based on their own reward functions.&lt;br /&gt;
However, once an option has been trained, the role for its intrinsic reward function is over.&lt;br /&gt;
A higher-level agent that uses an option sees it as just another action and evaluates it using its own reward function.&lt;br /&gt;
This can yield great speed-ups in learning and help substaintially with better exploration, but they do not directly make the value function of the higher-level agent less complex.&lt;/p&gt;

&lt;p&gt;Hierarchical RL:&lt;br /&gt;
[Barto and Mahadevan 2003] - Recent advances in hierarchical reinforcement learning&lt;br /&gt;
[Kulkarni el al 2016] - Hierarchical deep reinforce- ment learning: Integrating temporal abstraction and intrinsic motivation&lt;/p&gt;

&lt;h3 id=&quot;horde-architecture&quot;&gt;Horde architecture&lt;/h3&gt;

&lt;p&gt;How to learn, represent, and use knowledge of the world in a general sense remains a key open problem in AI.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;high-level representation: based on first-order predicate logic and Bayes network that are very expressive, but difficult to learn and computationally expensive to use&lt;/li&gt;
  &lt;li&gt;low-level representation: i.e. equations and state-transition matrices that can be learned from data without supervision, but less expressive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There remains room for exploring alternate formats for knowledge that are expressive yet learnable from unsupervised sensori-motor data&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Knowledge representation&lt;/strong&gt; based on the notion of value functions&lt;br /&gt;
Knowledge is represented as a large number of approximate value functions learned in parallel&lt;br /&gt;
each with its own policy, pseudo-reward function, pseudo-termination function, and pseudo-terminal-reward function&lt;/p&gt;

&lt;p&gt;Related:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;options [Sutton et al 2006] [Sutton et al 1999], explored as temporal-difference networks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gradient-descent temporal-difference algorithms&lt;br /&gt;
[sutton et al 2009,2008] - A convergent O(n) algorithm for off-policy temporal-difference
learning with linear function approximation
[Maei et al 2009,2010] - Convergent temporaldifference learning with arbitrary smooth function approximation, Toward off-policy learning control with function approximation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Off-policy experience&lt;/strong&gt; means experience generated by a policy called the &lt;strong&gt;behavior policy&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Behavior policy is different from that being learned about, called &lt;strong&gt;target policy&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;One wants to learn in parallel about many policies - the different target policy π of each GVF - but of course one can only behave according to one policy.&lt;/p&gt;

&lt;p&gt;For a typical GVF, the actions taken by the behavior policy will match its target policy only on occasion, and rarely for more than a few steps in a row.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For efficient learning, we need to be able to learn from these snippets of relevant experience, and this requires off-policy learning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The on-policy learning would require learning only from snippets that are complete in that the actions match those of the GVF’s target policy all the way to pseudo-termination, a much less common occurrence.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If learning can be done off-policy from incomplete snippets of experience then it can be massively parallel and potentially much faster than on-policy learning.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-independently-obtainable-reward-functions&quot;&gt;Learning Independently-Obtainable Reward Functions&lt;/h3&gt;

&lt;p&gt;2019&lt;/p&gt;

&lt;p&gt;proposed a method learning a set of disentangled reward functions that sum to the original env reward and are constrained to be independently obtainable.&lt;/p&gt;

&lt;p&gt;state decomposition:&lt;/p&gt;

&lt;p&gt;many goals RL [Kaelbling 1993] - Learning to achieve goals&lt;/p&gt;

&lt;p&gt;[Laversannne-Finot et al 2018] - Curiosity driven exploration of learned disentangled goal spaces&lt;/p&gt;

&lt;p&gt;decomposition of env state and learning corresponding control policies are separate processes&lt;/p&gt;

&lt;p&gt;[Thomas et al 2017] - Independently controllable features&lt;/p&gt;

&lt;p&gt;pairs together components of the learned state representation with control policies and measures the degree to which policies can control their corresponding components independently of other components&lt;/p&gt;

&lt;p&gt;They leverage some notion of disentanglement to address RL problems, but they didn’t take into account the reward function.&lt;/p&gt;

&lt;p&gt;=&amp;gt; &lt;strong&gt;Reward Decomposition&lt;/strong&gt;:&lt;br /&gt;
rewards are functions of state and have corresponding policies and hence decomposing rewards also implicitly decomposes states as well as policies.&lt;/p&gt;

&lt;p&gt;Env decompositions:&lt;br /&gt;
[Guestrin et al 2002] - Multiagent planning with factored mdps&lt;br /&gt;
[Kok and Vlassis 2006] - Collaborative multiagent reinforcement learning by payoff propagation&lt;br /&gt;
[Hu et al 1998] - Multiagent reinforcement learning: theoretical framework and an algorithm&lt;/p&gt;

&lt;p&gt;Reward factorization:&lt;br /&gt;
[Van Seijen et al 2017] - HRA&lt;br /&gt;
[Russell and Zimdars 2003] - Q-decomposition&lt;/p&gt;

&lt;h3 id=&quot;distributional-reward-decomposition-for-reinforcement-learning&quot;&gt;Distributional Reward Decomposition for Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;2019 nips&lt;/p&gt;

&lt;p&gt;multiple reward channel&lt;/p&gt;

&lt;p&gt;exisiting reward decomposition methods requires:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;prior knowledge of the env&lt;/li&gt;
  &lt;li&gt;without prior knowledge but with degraded performance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reward decomposition views the total reward as the sum of sub-rewards that are usually disentangled and can be obtained independently&lt;/p&gt;

&lt;p&gt;[Sprague and Ballard 2003] - Modular Sarsa (0)&lt;br /&gt;
[Russell and Zimdars 2003] - Q decomposition &lt;br /&gt;
[Van Seijen et al 2017] - HRA&lt;br /&gt;
(-) require prior knowledge&lt;br /&gt;
[Grimm and Singh 2019] - Learning Independently-Obtainable Reward Functions&lt;br /&gt;
(-) requires the env can be reset to arbitrary state and cannot apply to general RL setting where states can hardly be revisited&lt;br /&gt;
(-) despite the meaningful reward decomposition they achieved, they fail to utilize the reward decomposition into learning better policies&lt;/p&gt;

&lt;p&gt;The sub-rewards may further be leveraged to learn better policies&lt;/p&gt;

&lt;p&gt;propose &lt;strong&gt;Distributional Reward Decomposition&lt;/strong&gt; for RL:&lt;br /&gt;
captures the latent multiple-channel structure for reward, under the setting of distributional RL&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributional RL&lt;/strong&gt; estimates the distribution rather than the expectation of returns, and therefore captures richer information than value-based RL&lt;/p&gt;

&lt;p&gt;propose an RL algorithm that estimates distributions of the sub-returns and combine the sub-returns to get the distribution of the total returns&lt;/p&gt;

&lt;p&gt;To avoid naive decomposition such as 0-1 or half-half, further propose a disentanglement regularization term to encourage the sub-returns to be diverged&lt;/p&gt;

&lt;p&gt;Learn Different state representations for different channels&lt;/p&gt;

&lt;p&gt;State Decomposition:&lt;br /&gt;
[Laversannne-Finot et al 2018]  &lt;br /&gt;
[Thomas et al 2017]&lt;/p&gt;

&lt;p&gt;Distributional RL:&lt;br /&gt;
C51 - [Bellemare et al 2017] - A distributional perspective on reinforcement learning&lt;/p&gt;

&lt;p&gt;Horde - [Sutton et al 2011]&lt;br /&gt;
UVFA - [Schaul et al 2015] - Universal value function approximators&lt;/p&gt;

&lt;h3 id=&quot;behavior-coordination-for-a-mobile-robot-using-modular-reinforcement-learning&quot;&gt;Behavior Coordination for a Mobile Robot Using Modular Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;the prominence of RL role is largely dependent on the extent to which the learning can be scaled to solve larger and more complex robot learning tasks.&lt;/p&gt;

&lt;p&gt;[Singh] [11]&lt;br /&gt;
[Whitehead et al] [14]&lt;br /&gt;
[Connel and Mahadevan] [5]&lt;br /&gt;
[Gachet et al] [7]&lt;/p&gt;

&lt;p&gt;Existing methods explained above assume that the subtask state spaces do not interfere with each other or they are completely independent of each other. This assumption is too idealized and often does not hold in real robot tasks.&lt;/p&gt;

&lt;p&gt;[Asada et al ] [3]&lt;/p&gt;

&lt;p&gt;proposed a method for behavior coordination in a case that the subtask state spaces interfere with each other, and they applied it to real soccer robots.&lt;/p&gt;

&lt;p&gt;propose&lt;/p&gt;

&lt;p&gt;1) state space is classified into 2 categories based on the action values separately obtained by Q-learning&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;no more learning area - where one of the learned behaviors is directly applicable&lt;/li&gt;
  &lt;li&gt;re-learning area - learning is necessary due to the competition of multiple Behaviors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) hidden states are detected by model fitting to the learned action values based on the information criterion&lt;/p&gt;

&lt;p&gt;3) the initial action values in the re-learning area are adjusted so that they can be consistent with the values in the no more learning area&lt;/p&gt;

&lt;p&gt;previous methods for multi-goal:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;simple addition Q(s,a)=sum(Qi(s,a))&lt;/li&gt;
  &lt;li&gt;simple switching Q(s,a)=i_Qi(s,a)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These methods cannot cope with local maxima and/or hidden states caused by combination of state spaces. Consequently, an action suitable for these situations has never been learned&lt;/p&gt;

&lt;p&gt;To cope with these new situations, the robot needs to learn a new behavior by using the previously learned behaviors&lt;/p&gt;

&lt;p&gt;proposed:&lt;/p&gt;

&lt;p&gt;1) construct a new combined state space&lt;br /&gt;
2) learn a new behavior in the new state space&lt;/p&gt;

&lt;p&gt;Q(s,a) for normal states s&lt;br /&gt;
Q(s_sub,a) for new sub-states&lt;br /&gt;
Q initial = sum(Q)&lt;/p&gt;

&lt;p&gt;conservative strategy is used around the normal states&lt;br /&gt;
high random strategy around the new sub-states&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hidden states&lt;/strong&gt;:&lt;br /&gt;
aka inconsistent states&lt;br /&gt;
hidden states prevent the learning robot from acquiring an optimal behavior, therefore the robot should be able to find hidden states autonomously&lt;/p&gt;

&lt;p&gt;experiment: shoot a ball into a goal without collisions with a keeper robot&lt;/p&gt;

&lt;p&gt;two sub-tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;shoot a ball into the goal&lt;/li&gt;
  &lt;li&gt;avoid a moving keeper robot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;then coordinate&lt;/p&gt;

&lt;p&gt;[11] S. P. Singh. Transfer of Learning by Composing So- lution of Elemental Sequential Tasks. In Machine Learning, Vol. 8, pp. 99–115, 1992.&lt;br /&gt;
[14] S. D. Whitehead, J. Karlsson, and J. Tenenberg. Learning Multiple Goal Behavior Via Task Decomposition And Dynamic Policy Merging. In Connel and Mahadevan [6], chapter 3.&lt;br /&gt;
[5] J. H. Connel and S. Mahadevan. Rapid Task Learning for Real Robot. In Robot Learning [6], chapter 5, pp. 105–140.&lt;br /&gt;
[7] D. Gachet, M. A. Salichs, L. Moreno, and J. R. Pi- mentel. Learning Emergent Tasks for an Autonomous Mobile Robot. In Proc. of the 1994 IEEE/RSJ In- ternational Conference on Intelligent Robots and Sys- tems, pp. 290–297, 1994.&lt;/p&gt;

&lt;p&gt;[3] M. Asada, E. Uchibe, S. Noda, S. Tawaratsumida, and K. Hosoda. Coordination Of Multiple Behaviors Acquired By A Vision-Based Reinforcement Learn- ing. In Proc. of the 1994 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vol. 2, pp. 917–924, 1994.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;[1] Modular Deep Reinforcement Learning with Temporal Logic Specifications, 2019&lt;br /&gt;
[2] Modular Reinforcement Learning An Application to a Real Robot Task, 1997&lt;br /&gt;
[3] Composable Modular Reinforcement Learning, 2019&lt;br /&gt;
[4] Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation, 2016&lt;br /&gt;
[5] Hierarchical Reinforcement Learning: Learning sub-goals and state-abstraction, 2011&lt;br /&gt;
[6] Multiple Model-based Reinforcement Learning, 2002&lt;br /&gt;
[7] Policy Reuse in Reinforcement Learning for Modular Agents, 2019&lt;br /&gt;
[8] On the Difficulty of Modular Reinforcement Learning for Real-World Partial Programming, 2006&lt;br /&gt;
[9] Multiobjective Reinforcement Learning: A Comprehensive Overview, 2014  &lt;br /&gt;
[10] A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation, 2019&lt;br /&gt;
[11] Hybrid Reward Architecture for Reinforcement Learning, 2017&lt;br /&gt;
[12] Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction, 2011&lt;br /&gt;
[13] Learning Independently-Obtainable Reward Functions, 2019&lt;br /&gt;
[14] Distributional Reward Decomposition for Reinforcement Learning, 2019&lt;br /&gt;
[?] Global Policy Construction in Modular Reinforcement Learning, 2015&lt;/p&gt;

&lt;h3 id=&quot;additional&quot;&gt;Additional&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Convergence guarantee of Q-learning and SARSA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If in the limit the Q values of all admissible state-action pairs are updated infinitely often, and α decays in a way satisfying the usual stochastic approximation conditions, then the Q values will converge to the optimal value Q* with probability 1 [20].&lt;/p&gt;

&lt;p&gt;For the Sarsa algorithm, if each action is executed infinitely often in every state that is visited infinitely often, the action is greedy with respect to the current Q value in the limit, and the learning rate decays appropriately, then the estimated Q values will also converge to the optimal value Q* with probability 1 [21].&lt;/p&gt;

&lt;p&gt;[20] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the convergence
of stochastic iterative dynamic programming algorithms,” Neural
Comput., vol. 6, no. 6, pp. 1185–1201, Nov. 1994.
[21] S. P. Singh, T. Jaakkola, M. L. Littman, and C. Szepesvari,
“Convergence results for single-step on-policy reinforcement learning
algorithms,” Mach. Learn., vol. 38, no. 3, pp. 287–308, Mar. 2000.&lt;/p&gt;</content><author><name>Jiexin Wang</name></author><category term="Reinforcement Learning" /><summary type="html">Modular Deep Reinforcement Learning with Temporal Logic Specification</summary></entry></feed>